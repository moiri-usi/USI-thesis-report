\documentclass[mscthesis]{usiinfthesis}
\usepackage{lipsum}
\usepackage{color}
\usepackage{listings}
\usepackage{tikz}
\usepackage{pstricks}
\usepackage{auto-pst-pdf}
\usepackage{array}
\usepackage{colortbl}
\usepackage{multirow}
\usepackage{subscript}
\usetikzlibrary{calc}

% customizations
\definecolor{mygreen}{rgb}{0,0.4,0}
\definecolor{mygray}{rgb}{0.5,0.5,0.5}
\definecolor{mymauve}{rgb}{0.58,0,0.82}
\definecolor{mysmokegray}{rgb}{0.9,0.9,0.9}

%\newtoggle{InString}{}% Keep track of if we are within a string
%\togglefalse{InString}% Assume not initally in string
%
%\newcommand*{\ColorIfNotInString}[1]{\iftoggle{InString}{#1}{\color{mymauve}#1}}%
%\newcommand*{\ProcessQuote}[1]{#1\iftoggle{InString}{\global\togglefalse{InString}}{\global\toggletrue{InString}}}%
%\lstset{literate=%
%    {"}{{{\ProcessQuote{"}}}}1% Disable coloring within double quotes
%    {'}{{{\ProcessQuote{'}}}}1% Disable coloring within single quote
%    {0}{{{\ColorIfNotInString{0}}}}1
%    {1}{{{\ColorIfNotInString{1}}}}1
%    {2}{{{\ColorIfNotInString{2}}}}1
%    {3}{{{\ColorIfNotInString{3}}}}1
%    {4}{{{\ColorIfNotInString{4}}}}1
%    {5}{{{\ColorIfNotInString{5}}}}1
%    {6}{{{\ColorIfNotInString{6}}}}1
%    {7}{{{\ColorIfNotInString{7}}}}1
%    {8}{{{\ColorIfNotInString{8}}}}1
%    {9}{{{\ColorIfNotInString{9}}}}1
%}

\lstset{
    backgroundcolor=\color{mysmokegray},
    basicstyle=\footnotesize\ttfamily,
    breaklines=true,
    captionpos=b,
    commentstyle=\itshape\color{mygreen},
    escapeinside={//*}{\^^M},
    keywordstyle=\bfseries\color{blue},
    linewidth=0.95\linewidth,
    mathescape=true,
    numbers=left,
    numberstyle=\tiny\color{mygray},
}
\DeclareMathOperator{\erf}{erf}
\lstdefinelanguage{algebra}
{morekeywords={import,sort,constructors,observers,transformers,axioms,if,
else,end},
sensitive=false,
morecomment=[l]{//s},
}

%===============================================================================
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\title{Accelerator for Event-based Failure Prediction} %compulsory
\specialization{Embedded Systems Design}%optional
\subtitle{Acceleration of an Extended Forward Algorithm for Failure Prediction
    on FPGA}
\author{Simon Maurer} %compulsory
\begin{committee}
    \advisor{Prof.}{Miroslaw}{Malek} %compulsory
    %\coadvisor{Prof.}{Student's}{Co-Advisor}{} %optional
\end{committee}
\Day{29.} %compulsory
\Month{Janaury} %compulsory
\Year{2014} %compulsory, put only the year
\place{Lugano} %compulsory

%\dedication{To my beloved} %optional
%\openepigraph{Someone said \dots}{Someone} %optional

%\makeindex %optional, also comment out \theindex at the end

\begin{document}

\maketitle %generates the titlepage, this is FIXED

\frontmatter %generates the frontmatter, this is FIXED

\begin{abstract}
\end{abstract}

%\begin{abstract}[Zusammenfassung]
%optional, use only if your external advisor requires it in his/er
%language 
%\\
%
%\lipsum
%\end{abstract}

\begin{acknowledgements}
\end{acknowledgements}

\tableofcontents 
\listoffigures %optional
\listoftables %optional
\lstlistoflistings

\mainmatter

%===============================================================================
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Introduction}
\label{ch:intro}
In today's live it becomes increasingly important, that computer systems are
dependable. The reason being, that computer systems are used more and more in
areas where the failure of such a system can lead to catastrophic events.
Banking, public transportation and medical engineering are only a few examples
of areas employing large and extremely complex systems. The increasing
complexity of computer systems has a direct impact on their maintainability and
testability. It is simply impossible to guarantee that a piece of software comes
without any faults. On top of that, the same problematic arises with the
hardware components which also may contain faulty parts but also get
increasingly prone to failures due to decay of material.

In the event of a system failure it is of course desirable to fix the system as
soon as possible in order to minimize the downtime of the system (maximize the
availability). This can be accomplished by using different types of recovery
techniques, e.g. check-pointing (create checkpoints to roll back/forward), fail
over (switch to a redundant system), reboot. All these techniques require
a certain amount of time to complete the recovery process, time that is very
expensive. In order to minimize this time, techniques have been developed to
anticipate upcoming failures. Such a technique is described in
\cite{salfner08}.

The work presents a new algorithm to predict failures and compares the results
with other techniques. The accuracy of the presented algorithm to predict
failures proves to be better compared to the other techniques, has however the
drawback of increased complexity and hence increased computation time. It is
very important to keep the computation overhead very low in order to maximize
the time between the prediction of a failure and the actual event of the
failure. One way to decrease the computation time is to design a hardware
accelerator for the prediction algorithm. The design of such an accelerator is
outlined in this document.

%-------------------------------------------------------------------------------
%===============================================================================
\section{Problem Statement}
\label{ch:_intro_prob}

The main idea of the prediction model proposed in the reference work
\cite{salfner08} is to predict failures, based on sequences of events and their
time of arrival. This is modeled with a HSMM (Hidden Semi-Markov Model). A very
detailed description can be found in the reference work, but the fundamental
concept is also described here, in chapter \ref{ch:event}. In order to perform
a prediction, two parts are required: the training of the model and the
sequence processing. The training of the model uses collected error event data
samples and corresponding oracle predictions (collected from known failures) to
train the features (parameters) of the model in order to be able to detect
similar sequences as the ones that led to failures in previous cases. The
training of the model is not time critical and is performed off-line. The
trained features are then used in the second part, the sequence processing.
This is now time critical as this happens on-line on a live system. The system
will continuously send error events to the sequence processing unit, which
needs to compute a likelihood of the sequence of the last $L$ events. This
likelihood is sent to a classifier where it is compared to a non-failure
likelihood. The classifier then predicts if the sequence at hand will lead to
a failure in the near future.

The failure prediction method, as it was outlined in very few words above and
discussed in \cite{salfner08}, leads to very good prediction results (F-measure
of up to 0.66. See chapter \ref{ch:event} for more information). This comes at
the price of high computational efforts that are necessary to perform the
sequence processing ($O(LN^2)$). This may lead to the situation that a failure
is correctly predicted, but the result of the prediction is only available
after the failure has already happened. This would of course rend the
prediction useless. The main goal of this thesis, aims to accelerate the
sequence processing part in order to predict a failure before it really
happens. To do so, first available parallelism and an appropriate accelerator
architecture must be found. Then the accelerator must be designed and
implemented, using the knowledge gained from the analysis and finally the
implementation must be compared to a serial implementation and a speedup and
accuracy must be computed.

%-------------------------------------------------------------------------------
%===============================================================================
\section{Motivation}
\label{ch:intro_mot}
The email of Felix left some doubts to whether the acceleration of the
algorithm is useful. The following list will give some arguments to justify
the work.
\begin{description}
    \item[Too many parameters to be identified, estimated and set] \hfill \\
        Considering an embedded system, this is usually not a problem because
        the parameters are defined during the design phase and will never be
        changed afterwards.
    \item[Limited performance scalability] \hfill \\
        There are studies available claiming otherwise. The discussion of
        Neumanns work will provide some arguments against this statement.
    \item[Industry trends point towards cloud] \hfill \\
        In embedded systems it will still be beneficial to predict failures of
        single nodes. It is however important to keep the power and
        computational footprint low. This will be one of the major challenges.
        On the other hand, I think it would also be possible to also use this
        algorithm to monitor a distributed system and predict failures. It is
        only a matter of getting defining the events to feed to the algorithm.
\end{description}

%-------------------------------------------------------------------------------
%===============================================================================
\section{Structure}
\label{ch:intro_struct}



%===============================================================================
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{State of the Art}
\label{ch:art}
This section provides an overview of the state of the art in the different
fields of research that are relevant for the thesis. This includes failure
prediction methods, existing solutions to accelerate failure prediction
algorithms and acceleration techniques in general.

%-------------------------------------------------------------------------------
%===============================================================================
\section{Failure Prediction}
\label{ch:art_pred}
A very detailed overview of failure prediction methods is given in
\cite{ACM10_Salfner}. The survey discusses i.a. the techniques used as
comparison in the main reference
\cite{lin88,IEEE90_lin,ICDM02_Vilalta,domeniconi02} as well as the technique
described in the main reference \cite{salfner08}.

More recent work uses hardware counters of a general purpose CPU and combines
them with software instrumentation to analyze failures of single processes (e.g
grep, flex, sed) \cite{FSE10_Yilmaz}. As industry heads more and more
towards cloud computing, it has been proposed to use information of interaction
between nodes (instead of analyzing single nodes) in order to analyze and
predict failures of a distributed system \cite{IEEE12_Salfner,DSN10_Oliner}.

%-------------------------------------------------------------------------------
%===============================================================================
\section{Accelerator}
\label{ch:art_acc}
The main goal of this master thesis is to accelerate an adaptation of the
forward algorithm. Proposals for a GPU based accelerator for the classic
forward algorithm are described in \cite{neumann11,liu09}. Further, several
proposals to accelerate the Viterbi algorithm (which is closely related to the
forward algorithm) have been published: \cite{ASAP12_Azhar} presents an
architecture for a lightweight Viterbi accelerator designed for an embedded
processor datapath, \cite{IPDPS07_Jacob,ICS06_Maddimsetty,IPDPS07_Oliver}
describe a FPGA based accelerator for protein sequence HHM search and
\cite{IPDPS09_Walters} describes i.a. an approach to accelerate the Viterbi
algorithm from the HMMER library using GPUs.

Focusing on a more general approach for acceleration, \cite{ARITH13_Kadric}
proposes an FPGA implementation of a parallel floating point accumulation and
\cite{ITNG07_Yang} describes the implementation of a vector processor on
FPGA.

Quite some research has been done on the question what type of technology
should be used to accelerate certain algorithms: \cite{SASP08_Che} presents
a performance study of different applications accelerated on a multicore CPU,
on a GPU and on a FPGA, \cite{FPL10_Jones} discusses the suitability of FPGA
and GPU acceleration for high productivity computing systems (HPCS) without
focusing on a specific application and \cite{ISVLSI10_Kestur} also focuses on
HPCS but uses the Basic Linear Algebra Subroutines (BLAS) as comparison and
also takes CPUs into account.

It may be interesting to also think about an acceleration of the model
training. Similar work has been done by accelerating SVMs (Support Vector Machines):
\cite{FCCM09_Cadambi} describes a FPGA based accelerator for the SVM-SMO
(support vector machine - sequential minimal optimization) algorithm used in
the domain of machine learning and \cite{IEEE03_Anguita} proposes a new algorithm
and its implementation on a FPGA for SVMs.

%===============================================================================
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Event-based Failure Prediction}
\label{ch:event}

\emph{\color{red} describe complete concept in a brief overview: non-failure
model, failure models, classification}

This section provides a brief overview of the computational steps done by the
proposed algorithm \cite{salfner08}.

\emph{\color{red}brief description of the idea behind the algorithm, HSMM, Events, etc}

To be able to understand the formal expression of the algorithm, first
a definition of the used parameters is provided.
\begin{itemize}
    \item N: number of states
    \item M: number of observation symbols
    \item L: observation sequence length
    \item R: number of cumulative probability distributions (kernels)
\end{itemize}
The delay of the event at time $ t_k $ with respect to the event at time
$ t_{k-1} $ is described as
\begin{equation}
\label{eq:delay}
    d_k = t_k-t_{k-1}
\end{equation}

%-------------------------------------------------------------------------------
%===============================================================================
\section{Data Processing}
\label{ch:event_data}

%-------------------------------------------------------------------------------
%===============================================================================
\section{Training of the Model}
\label{ch:event_train}

One part of the algorithm is the model training. This part is not described
here. The features to be trained by the model training are however important
in this context because they are used by the adapted forward algorithm.
Following the features:
\begin{itemize}
    \item $ \pi_i $, forming the initial state probability vector
        $ \boldsymbol{\pi} $ of size $ N $
    \item $ b_i(o_j) $, forming the emission probability matrix $ B $ of size
        $ N \times M $
    \item $ p_{ij} $, forming the matrix of limiting transmission probabilities
        $ P $ of size $ N \times N $
    \item $ \omega_{ij, r} $, the weights of the kernel $ r $
    \item $ \theta_{ij, r} $, the parameters of the kernel $ r $
\end{itemize}

%-------------------------------------------------------------------------------
%===============================================================================
\section{Sequence Processing}
\label{ch:event_sequ}

The following description will provide a complete blueprint of the extended
forward algorithm, that allows to implement it, but without any explanations or
proofs related to the formulation. The adapted forward algorithm is defined as
follows:
\begin{equation}
    \label{eq:forward_init}
    \alpha_0(i) = \pi_{i}b_{s_i}(O_0) \\
\end{equation}
\begin{equation}
    \label{eq:forward}
    \alpha_k(j) = \sum_{i=1}^{N} \alpha_{k-1}(i) v_{ij}(d_k) b_{s_j}(O_k);
    \quad 1 \leq k \leq L
\end{equation}
where
\begin{equation}
    \label{eq:V}
    v_{ij}(d_k) = \left\{
        \begin{array}{l l}
            p_{ij} d_{ij}(d_k)
                & \quad \text{if $j \neq i$}\\
            1 - \sum\limits_{\substack{h=1 \\ h \neq i}}^{N} p_{ih} d_{ih}(d_k)
                & \quad \text{if $j = i$}
        \end{array} \right.
\end{equation}
with
\begin{equation}
    \label{eq:D}
    d_{ij}(d_k) = \sum_{r=1}^{R} \omega_{ij,r}\kappa_{ij,r}(d_k|\theta_{ij, r})
\end{equation}
forming the matrix of cumulative transition duration distribution functions
$ D(d_k) $ of size $ N \times N \times L $.

For simplification reasons, only one kernel is used. Due to this, the kernel
weights can be ignored. Equation \ref{eq:D} can then be simplified:
\begin{equation}
    \label{eq:D_fact}
    d_{ij}(d_k) = \kappa_{ij}(d_k | \theta_{ij})
\end{equation}
Choosing the Gaussian cumulative distribution results in the kernel parameters
$ \mu_{ij} $ and $ \sigma_{ij} $:
\begin{equation}
    \label{eq:kernel}
    \kappa_{ij, gauss}(d_k | \mu_{ij}, \sigma_{ij}) = 
    \frac{1}{2}\bigg [1 + \erf \big (\frac{d_k - \mu_{ij}}{\sqrt 2 \sigma_{ij}}\big )
        \bigg ]
\end{equation}

\emph{\color{red}explain difference to the non-extended forward algorithm and
    introduce a standard notation for the transition probabilities (eg v: extended
a: basic, tp: general) stick to this in the whole document}

The last set of forward variables $ \alpha_L $ are then summed up to compute
a probabilistic measure for the similarity of the observed sequence compared to
the sequences in the training data set. This is called the sequence likelihood:
\begin{equation}
    \label{eq:P}
    P(\boldsymbol{o}|\lambda) = \sum\limits_{i=1}^{N} \alpha_L(i)
\end{equation}
where $ \lambda = \{\boldsymbol{\pi}, P, B, D(d_k) \} $.

To prevent $ \alpha $ from going to zero very fast, at each step of the forward
algorithm a scaling is performed:
\begin{equation}
    \label{eq:scaled}
    \alpha_k(i) = c_k \alpha_k(i)
\end{equation}
with
\begin{equation}
    \label{eq:scaling_factor}
    c_k = \frac{1}{\sum\limits_{i=1}^{N} \alpha_k(i)}
\end{equation}

By applying scaling, instead of the sequence likelihood (equation \ref{eq:P}),
the sequence log-likelihood must be computed:
\begin{equation}
    \label{eq:Plog}
    \log(P(\boldsymbol{o}|\lambda)) = -\sum\limits_{k=1}^{L} \log c_k
\end{equation}
where $ \lambda = \{\boldsymbol{\pi}, P, B, D(d_k) \} $.

%-------------------------------------------------------------------------------
%===============================================================================
\section{Classification}
\label{ch:event_class}

\emph{\color{red}explain classification}

and finally the
classification is performed:
\begin{equation}
    \label{eq:class}
    \text{class}(s) = F \iff \max_{i=1}^{u} \big [
        \log P(\boldsymbol{s}|\lambda_i)
    \big ] - \log P(\boldsymbol{s}|\lambda_0) > \log \theta
\end{equation}
with
\begin{equation}
    \label{eq:class_thresh}
    \theta = \frac{(r_{\bar{F}F} - r_{\bar{F}\bar{F}})P(c_{\bar{F}})}
        {(r_{F \bar{F}} - r_{FF})P(c_{F})}
\end{equation}

\emph{\color{red}classification without scaling}

\emph{\color{red}Multi-class classification without scaling?}

To calculate $ \theta $, the following parameters need to be set:
\begin{itemize}
    \item $ P(c_{\bar{F}}) $: prior of non-failure class
    \item $ P(c_F) $: prior of failure class
    \item $ r_{\bar{F}\bar{F}} $: true negative prediction
    \item $ r_{FF} $: true positive prediction
    \item $ r_{\bar{F}F} $: false positive prediction
    \item $ r_{F\bar{F}} $: false negative prediction
\end{itemize}

%-------------------------------------------------------------------------------
%===============================================================================
\section{Metrics}
\label{ch:event_metrics}

\emph{\color{red}cite results from Felix}

%===============================================================================
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Theoretical Analysis of the Forward Algorithm}
\label{ch:analysis}

This chapter provides details about the forward algorithm and available (and
useful) parallelization techniques applicable on the algorithm. The generally
known forward algorithm as well as the extended version proposed in the
reference work is discussed. Further, scaling techniques of the forward
variables and their impact on data representation choices are presented.
Finally the observations presented in an overview and an appropriate choice on
possible acceleration hardware is made.

%-------------------------------------------------------------------------------
%===============================================================================
\section{Serial Implementation and Complexity}
\label{ch:analysis_serial}

The sequential implementation of the basic forward algorithm is represented in
listing \ref{list:forward_basic}. It consist of three parts: the initialization
step, the computation of consecutive forward variables and the final step,
where the likelihood is computed. The initial $ \alpha $ variable is computed
by multiplying the initial state probability with the emission probability of
the first observation symbol of the sequence (cf equation
\ref{eq:forward_init}). The computation of the following forward variables
consists of three nested loops: the outer loop iterates over the $ L $ sets of
$ N $ $ \alpha $ variables, where each variable depends on the prior computed
$ \alpha $ variable and the k-th observation symbol of a sequence. The first
inner loop iterates over the $ N $ $ \alpha $ variables of one set, where each
variable is computed with the most inner loop. The two nested inner loops form
the Matrix-Vector-Vector multiplication
\begin{equation}
    \label{eq:mvv}
    \alpha_{k+1} = TP * \alpha_k \cdot B(o_k)
\end{equation}
where $ \alpha_k $ is a vector of size $ N $ of the prior computed $ \alpha
$ variables, $ TP $ a matrix of size $ N \times N $ containing the transition
probabilities and $ B(o_k) $ a vector of size $ N $ containing the emission
probabilities of the k-th observation symbol. Note that the first
multiplication is a Matrix-Vector multiplication that results in a vector,
which is then multiplied element-wise with the vector $ B(o_k) $. The equation
\ref{eq:forward} describes the formal definition of the forward algorithm. In
the last step the likelihood is computed, by summing up all elements of the
last forward variable $ \alpha_L $ (cf equation \ref{eq:P}).

\lstinputlisting[language=Octave,
    caption=Forward Algorithm,
    float,
    label=list:forward_basic]
    {../accelerator/model/forward_s_basic.m}

As proposed by the reference work, the forward variables can be scaled, in
order to prevent the result from getting very small due to the continuous
multiplication of probabilities. The implementation of the proposed scaling
method is shown in listing \ref{list:forward_scaling}. The scaling is formally
defined by the equations \ref{eq:scaled} and \ref{eq:scaling_factor}. Due to the
scaling, instead of the likelihood, the log-likelihood is computed. Equation
\ref{eq:Plog} gives the formal definition.

\lstinputlisting[language=Octave,
    caption=Forward Algorithm with scaling,
    float,
    label=list:forward_scaling]
    {../accelerator/model/forward_s_scaling.m}

The algorithm to compute the sequence likelihood proposed by the reference work
is an extension to the forward algorithm presented in the listings
\ref{list:forward_basic} and \ref{list:forward_scaling}. Instead of constant
transition probabilities, the extended algorithm computes a new transition
probability matrix (size $ N \times N$) for each arriving observation symbol,
by considering the delay of the new symbol with respect to the prior symbol.
The computation of the transition probability matrix $ TP $ is implemented with
listing \ref{list:ext} and formally defined by the equations \ref{eq:V},
\ref{eq:D}, \ref{eq:D_fact} and \ref{eq:kernel}. As described in chapter
\ref{ch:event_sequ}, also here for reasons of simplification, only one kernel
is used. In the sample code the Gaussian cumulative distribution function is
used. The function needs to be called for every k.

\lstinputlisting[language=Octave,
    caption=Extension of the Forward Algorithm with only one kernel (Gaussian),
    float,
    label=list:ext]
    {../accelerator/model/compute_tp.m}

The order of time complexity of the algorithm is $O(LN^2)$. While the
complexity increases with the introduction of scaling and/or the extension, the
order of complexity stays the same. The same is true for the space complexity
which is of the order $O(N^2)$.

%-------------------------------------------------------------------------------
%===============================================================================
\section{Parallelism and Faisable Parallelization}
\label{ch:analysis_parallel}

By applying parallelization methods one aims to increase the throughput or
reduce the latency of a task, or to achieve both at the same time. This can be
done at the cost of increased usage of parallel computation units, memory and
memory bandwidth.

Considering only the basic forward algorithm (listing
\ref{list:forward_basic}), the computation of the likelihood is divided in
$L+1$ steps: the initialization, $L-1$ identical intermediate steps and the
finalization. Because of the recursive nature of the algorithm, all steps
(except the initialization) depend on the previously computed forward
variables. For this reason a direct parallelization of the steps is not
possible. However, at every arrival of a new observation symbol, the last $L$
elements of the observation symbol sequence are used to compute the likelihood
(cf. figure \ref{fig:sliding}). This can be exploited to pipeline the steps in
order to increase the throughput. By building a pipeline of $L+1$ stages, where
each step of the forward algorithm corresponds to a pipeline stage,
a likelihood is computed at every completion of a step, with a latency of
$(L+1)*t_{step_{max}}$, where $t_{step_{max}}$ is the time needed to complete
the computation of the most complex step (each stage of the pipeline must take
the same amount of clock cycles). The throughput of a pipelined compared to
a non-pipelined system is increased by factor $L$ (assuming an infinite runtime
or by ignoring the setup time). Another and more important fact, that makes the
pipeline architecture very beneficial in this particular case: the
configuration allows to load the transition probabilities $TP$ and the emission
probabilities $b_i(o_k)$ for all steps at the same time, which reduces the load
operations by factor $L$. This is visualized by the table \ref{tab:pipeline}.
The table shows the pipeline stages with input values that are fed to the stage
before the execution (note, that the input values $TP$ and $B$ always depend on
the same observation symbol. The parameter $d_k$ of the transition
probabilities can be ignored in this case, because only in the extended forward
algorithm the they depend on $d_k$. This will be discussed further when the
extension is considered) and the output values resulting after the execution of
the pipeline stage. Figure \ref{fig:pipeline} shows a schematic representation
of the pipeline.

\begin{figure}
    \centering
    \input{./schema/window.tex}
    \caption{Sliding window over an observation sequence of the
        last $L=10$ observation symbols}
    \label{fig:sliding}
\end{figure}

\begin{table}
    \footnotesize
    \begin{center}
    \begin{tabular}{|l|*{6}{c|}}
    \cline{3-7}
    \multicolumn{2}{c|}{} & \multicolumn{5}{c|}{Pipeline}\\
    \hline
    Symb & I/O & Init & Step 2 & \dots & Step L & Final \\
    \hline
    \hline
    $O_1$ & in
        & $B(O_1)$ & $B(O_1)$, $TP(d_1)$, 0
        & \dots
        & $B(O_1)$, $TP(d_1)$, 0 & 0 \\
        & out
        & $\alpha_1(O_1)$ & 0
        & \dots
        & 0 & 0 \\
    \arrayrulecolor{mysmokegray}\hline
    $O_2$ & in
        & $B(O_2)$ & $B(O_2)$, $TP(d_2)$, $\alpha_1(O_1)$
        & \dots
        & $B(O_2)$, $TP(d_2)$, 0 & 0 \\
        & out
        & $\alpha_1(O_2)$ & $\alpha_2(O_{1,2})$
        & \dots
        & 0 & 0 \\
    \hline
    \vdots & & \vdots & \vdots & & \vdots & \vdots \\
    \hline
    $O_{L}$ & in
        & $B(O_L)$ & $B(O_L)$, $TP(d_L)$, $\alpha_1(O_{L-1})$
        & \dots
        & $B(O_L)$, $TP(d_L)$, $\alpha_{L-1}(O_{1,\dots,{L-1}})$ & 0 \\
        & out
        & $\alpha_1(O_L)$ & $\alpha_2(O_{{L-1},L})$
        & \dots
        & $\alpha_L(O_{1,\dots,L})$ & 0 \\
    \hline
    $O_{L+1}$ & in
        & $B(O_{L+1})$ & $B(O_{L+1})$, $TP(d_{L+1})$, $\alpha_1(O_L)$
        & \dots
        & $B(O_{L+1})$, $TP(d_{L+1})$, $\alpha_{L-1}(O_{2,\dots,L})$ & $\alpha_L(O_{1,\dots,L})$ \\
        & out
        & $\alpha_1(O_{L+1})$ & $\alpha_2(O_{L,{L+1}})$
        & \dots
        & $\alpha_L(O_{2,\dots,{L+1}})$ & $Ps(O_{1,\dots,L})$ \\
    \hline
    \vdots & & \vdots & \vdots & & \vdots & \vdots \\
    \arrayrulecolor{black}\hline
    \end{tabular}
    \end{center}
    \caption{Pipelined Forward Algorithm, with observation symbol $O_k$ and its
        delay $d_k$. Here $O_{i, \dots, k}$ is a short notation for $O_i, \dots,
        O_k$}
    \label{tab:pipeline}
\end{table}

\begin{figure}
    \centering
    \input{./schema/pipeline.tex}
    \caption{Pipelined Forward Algorithm}
    \label{fig:pipeline}
\end{figure}

By considering all dissimilar steps of the forward algorithm, more
parallelization options can be found. In the initial step, $N$ components of
the first forward variable $\alpha_1$ are computed by multiplying independent
pairs of an initial state probability $\pi_i$ and an emission probability of
the first observation symbol $b_i(o_0)$. This can be fully parallelized by
replicating the multiplication operation $N$ times. Doing this results in
a increase of the throughput by factor $N$ and a decrease of the latency by
factor $\frac{1}{N}$, assuming that $N$ multipliers are available and the
memory bandwidth is able to provide a data throughput $N$ times higher than in
the sequential case (including the memory interface).

The computation of the following forward variables $\alpha_k$, with $k
= 2 \dots L$ are similar. To compute the $N$ elements of one step, the
Matrix-Vector-Vector multiplication described by equation \ref{eq:mvv} must be
performed. Considering first only the Matrix-Vector multiplication, this can be
parallelized by decomposing the matrix in to subsets and then use multiple
computational units to perform multiplication and/or accumulation operations in
parallel on the subsets. An intuitive decomposition can be done either by
block-striped matrix partitioning (decomposition into subsets of rows or
columns) or by checkerboard block matrix partitioning (decomposition in
rectangular sets of elements). These partitioning methods are shown in figure
\ref{fig:matrix_partitioning}. The number of subsets must correspond to the
number of available computational units to perform the necessary operations.
The choice of decomposition is heavily dependant on the accelerator
architecture (e.g.  communication between computational units, memory
architecture). The resulting vector can then be multiplied element wise by the
emission probability vector, which is again the same case as described above.
In case of the block-striped matrix partitioning, the maximal achievable
increase of the throughput is a factor of $N$ and the latency can be decreased
by factor $\frac{1}{N}$, assuming that $N$ computational units are available to
perform the multiplication and accumulation operation on each subset, $N$
multipliers to compute the final element wise vector-vector multiplication and
a memory interface, that can handle a data throughput that is $N$ times higher
than in the sequential case. The checkerboard partitioning yields a lower gain
but may be considered in case of fixed computational unit architecture (CPU,
GPU) in order to increase the utility of recourses available in each unit.
Apart from homogeneous partitioning methods as mentioned above also
inhomogeneous solutions have been proposed \cite{IPDPSW12_DeFlumere, clarke11}.
These are not considered in this work, as the focus lies on homogeneous
computation units.

\begin{figure}
    \centering
    \input{./schema/matrix.tex}
    \caption{Matrix partitioning (from left to right): column-block-striped,
        row-block-striped and checkerboard blocks}
    \label{fig:matrix_partitioning}
\end{figure}

Further parallelization can be done by using a reduction tree to accumulate the
elements in the matrix-vector multiplication process. Instead of using one
computation unit and accumulate the values sequentially, $N$ units can be used
to first multiply two operands together, then adding $\frac{N}{2}$ resulting
operand pairs in a second step and then consecutively adding resulting pairs
until only one value results. This process is visualized in figure
\ref{fig:red_tree}. The maximal increase of throughput is of factor $\log_2(N)$
and the latency can be decreased by factor $\frac{1}{\log_2(N)}$, assuming that
$N$ computation units are available and the memory interface is able to handle
a throughput that is $N$ times higher than in the sequential case.

\begin{figure}
    \centering
    \input{./schema/red_tree.tex}
    \caption{Reduction Tree with $N=5$}
    \label{fig:red_tree}
\end{figure}

The finalization step of the algorithm consists of calculating the likelihood.
This is done by simply accumulating the $N$ elements of the last forward
variable $\alpha_L$. This operation can be parallelized with a reduction tree,
resulting in a throughput and latency optimization as described above.

The following sections will describe the impact on performance if scaling or
the extension of the forward algorithm is implemented. Also the availability of
parallelization in both cases will be discussed. Everything will be concluded
with an overview of available parallelism and a discussion about the usefulness
of each parallelization method in the context of the different algorithm
implementations.

%-------------------------------------------------------------------------------
%===============================================================================
\section{Prediction Model Simplification}
\label{ch:analysis_simple}

Until now, it was always assumed, that the model is fully connected (ergodic),
i.e. that every state can be reached from every other state. This is not
necessarily the case, as it is often possible to describe a system with
a simpler model. By adding more constraints to the possible state transitions
(e.g. only one direction, feed-forward), only a few elements in the transition
probability matrix are non-zero. In this case it is beneficial to use an array
(adjacency list) instead of a matrix to represent the transition probabilities.
Different methods have been proposed on how to store sparse matrices, but they
are usually strongly dependant of the architecture and will hence be discussed
only after the type of acceleration device has been chosen. A list with only
non-zero elements instead of a sparse matrix reduces the necessary memory to
store the data and makes a lot of computations superfluous. A Matrix-Vector
multiplication parallelization as described in the previous section would not
be beneficial anymore as a lot of computational units would be idle in most of
the time.

%-------------------------------------------------------------------------------
%===============================================================================
\section{Scaling and Data Representation}
\label{ch:analysis_scaling}

Scaling may be applied to prevent that the continuous multiplication of numbers
smaller than one (e.g. probabilities) result in zero, because of the limited
accuracy by digitally representing fractional numbers. Scaling does not
influence the order of complexity of the algorithm. By introducing a scaling
method as proposed in the reference work, the complexity of calculating one
$\alpha_k$ vector goes from $N^2$ (no scaling) to $N^2 + 2N + 1$ (scaling),
which is the same order $O(N^2)$. However, the introduction of scaling may
increase the usage of recourses significantly: In order to scale $\alpha_k$,
the division operation is used to compute the scaling factor.  Division is far
more complex than multiplication and hence uses more recourses.  Additionally,
instead of the sequence likelihood (equation \ref{eq:P}) the sequence
log-likelihood (equation \ref{eq:Plog}) needs to be computed, with the even
more complex log operation.

In order to limit the amount of necessary division operations, it is beneficial
to consider the following: Rather than scaling each element of $\alpha_k$ by
dividing it by a scaling factor ($N$ divisions), first the inverse of the
scaling factor can be computed, which is then multiplied with each element of
$\alpha_k$ (one division and $N$ multiplications). Using $N$ multiplication
units, this operation can be parallelized.

To compute the log-likelihood, $N$ log and $N$ sum operations are necessary, in
comparison to $N$ sum operations for the likelihood. In terms of memory, the
log-likelihood is more complex because the scaling coefficients of each
$\alpha_k$ are used and need to be stored, while for the likelihood only the
last set of forward variables $\alpha_L$ are used. The computation of the
log-likelihood can be parallelized by using $N$ units computing the log
function and additionally by a reduction tree to speed up the accumulation.

Instead of using the proposed scaling method, a simpler scaling may be applied.
By analyzing the operands, an average scaling factor can be computed. Using the
knowledge, that all the operands are probabilities,

\begin{equation}
    \label{eq:scaling_sum}
    \sum\limits_{i=1}^{N} \pi_i = 1, \
    \sum\limits_{j=1}^{N} tp_{ij} = 1, \
    \sum\limits_{j=1}^{M} b_{ij} = 1
\end{equation}

and doing the computation of the forward variables,

\begin{equation}\begin{split}
    \label{eq:scaling_estimation}
    &\hat{\alpha}_1 = \hat{b} \cdot \hat{\pi} = \frac{1}{NM} \\
    &\hat{\alpha}_2 = N \cdot \hat{\alpha}_1 \cdot \hat{tp} \cdot \hat{b} =
        N \cdot \frac{1}{NM} \cdot \frac{1}{N} \cdot \frac{1}{M} =
        \frac{1}{NM^2} \\
    &\hat{\alpha}_3 = N \cdot \hat{\alpha}_2 \cdot \hat{tp} \cdot \hat{b} =
        N \cdot \frac{1}{NM^2} \cdot \frac{1}{N} \cdot \frac{1}{M} =
        \frac{1}{NM^3} \\
    &\vdots\\
    & \hat{\alpha}_L = \frac{1}{NM^L}
\end{split}\end{equation}

it can be computed, that assuming no precision loss at each computational step
$k$, on average a scaling factor of $\frac{1}{M}$ is necessary in each step
$k$. If the intermediate precision of the computational units is high enough to
compensate for scaling to much or to few, this method is an easy solution to
keep the values in an acceptable range. However, if the precision is not
available (eg. if a fixed point data representation is chosen) a fixed scaling
factor can cause an overflow (very bad because the result will be wrong) or an
underflow (may be acceptable because it is only a loss of precision). In this
case, rather than choosing an average scaling factor of $\frac{1}{M}$ it is
safer to choose the scaling factor to be equal to the maximal possible scaling
factor of all values of a specific event in $B$ (scale $\max\big(B(O_k)\big)$).
By doing this, the scaling factor will be to small and if $L$ is big, the
forward variables will still approach zero, only slower than without scaling.
This is either acceptable because of a high precision, or another scaling
factor must be computed to prevent this. The implemented solution will be
explained in detail in chapter \ref{ch:design}.

Another aspect to consider is the choice of data representation (floating point
versus fixed point). This depends on one hand on the necessary precision and on
the other hand on the choice of accelerator type. While general purpose
hardware such as CPU, GPU and DSP (to some degree) offer an abstraction to make
the representation type transparent to the developer, specialized hardware such
as FPGA or ASCI offer no such abstraction. For the later devices, floating
point operations increase the complexity of the hardware design and the
necessary hardware resources considerably. In terms of performance, general
purpose devices benefit also from a sparse usage of floating point values. The
complexity of the software development however is only marginally or not
affected at all by the choice of data representation.

If by choice, scaling is omitted, a fixed point representation will not be
possible, due to the rapid convergence towards zero by continuously multiplying
probabilities together. This implies, that by omitting scaling to save
resources, a floating point representation must be used, which again increases
the resource requirements or has a negative impact on performance (or both).

The trade-off between the choice of using scaling or not versus the choice of
the precision and the data representation, will be analyzed in more detail in
chapter \ref{ch:design}, when the technology of the accelerator has been
chosen.

%-------------------------------------------------------------------------------
%===============================================================================
\section{Extension of the Forward Algorithm}
\label{ch:analysis_extension}

The proposed extension uses a transition probability matrix which is not
constant. For every arriving observation symbol, the matrix must be recomputed
by using the delay of the symbol (equation \ref{eq:delay}) and the sum of
different cumulative distribution functions (equations \ref{eq:V} and
\ref{eq:D}). To compute $N^2$ cumulative distribution functions is very
expensive but it only needs to be computed once per $d_k$ and can then be
stored for later usage.  A transition probability matrix can be used for the
computation of $L$ forward variables due to the continuous computation of
likelihood values (as depicted in figure \ref{fig:sliding}). This implies, that
storage for $L$ such matrices must be available or the matrix must be
recomputed when needed and only the delay value is stored. In case of
a pipelined architecture, the additional storage or computation is not
necessary (cf. figure \ref{fig:pipeline} and table \ref{tab:pipeline}). The
computation of the transition probability matrix can be fully parallelized with
$R*N^2$ computation units to perform a cumulative distribution function of
$d_k$, where $R$ is the number of different cumulative distribution functions
necessary to model the behaviour of the events. The memory interface needs to
be able to provide a throughput that is $R*N^2$ higher than in the sequential
case (note that each cumulative distribution function takes several parameters
as input. Eg. the normal cdf has the two parameters $\mu$ and $\sigma$). In
previous chapters, $R$ was always assumed to be equal to one in order to
simplify the problem.

Note, that while the computation of one extended transition probability is
independent of $N$ or $L$, it is still a lot more complex than the simple
multiply-accumulate operation necessary to calculate a forward variable (cf.
the list below). Due to this, the computational units calculating the
transition probability matrix must provide a lot more performance than the
units necessary to compute the resulting forward variables in order to not
limit the throughput. To get a rough impression of how expensive the
computation of a distribution function is, the following list with three common
examples is provided:

\begin{description}
    \item[Exponential CDF] \hfill \\
        This distribution function describes the time between
        events in a Poisson process (events occur continuously and independently
        at a constant average rate). It is expressed as
        \begin{equation}
            \label{eq:V}
            F_{exp}(x) = \left\{
                \begin{array}{l l}
                    1 - \exp(-\lambda x)
                        & \quad \text{if $x \geq 0$}\\
                    0
                        & \quad \text{if $x < 0$}
                \end{array} \right.
        \end{equation}
        Only the exponential function is needed, which is quite a complex
        function compared to a multiplication and could be problematic in a
        fully parallelized implementation (considering all parallelization
        options), but realizable in an implementation with less parallelism.
    \item[Lapalce CDF] \hfill \\
        This distribution is somewhat the extension of the
        exponential distribution and is also called double exponential
        distribution as it can be described as two exponential distributions
        put together (one flipped horizontally). It is expressed as
        \begin{equation}
            \label{eq:V}
            F_{laplace}(x) = \left\{
                \begin{array}{l l}
                    1 - \frac{1}{2}\exp(-\frac{x-\mu}{b})
                        & \quad \text{if $x \geq \mu$}\\
                    \frac{1}{2} \exp(\frac{x-\mu}{b})
                        & \quad \text{if $x < \mu$}
                \end{array} \right.
        \end{equation}
        In terms of complexity, for the Laplace distribution holds the same as
        for the exponential CDF.
    \item[Gaussian CDF] \hfill \\
        This is a very important distribution that is used in a lot of
        applications. It is used for real-valued random variables whose
        distributions are not known. The Gaussian (normal) cumulative
        distribution function cannot be expressed in terms of elementary
        functions, which is the reason why the special error function $\erf$ is
        used. It is defined by equation \ref{eq:kernel}. Using integration by
        parts and the substitution $x = \frac{d_k-\mu}{\sigma}$, it can be
        expressed as
        \begin{equation}
            \Phi(x) = \frac{1}{2}
                + \frac{1}{\sqrt{2\pi}} \cdot \exp(-\frac{x^2}{2})
                \cdot \Bigg[ x+\frac{x^3}{3} + \frac{x^5}{3 \cdot 5} + \cdots
                + \frac{x^{2n+1}}{3 \cdot 5 \cdots (2n+1)} \Bigg]
        \end{equation}
        This computation is very expensive. It comprises of an exponential
        function and an iterative approach (to achieve the necessary precision)
        including the power function and additions. If this distribution is
        chosen to describe the time between events, parallelization will be very
        challenging, in order to prevent this calculation from being the
        bottleneck.
\end{description}

Considering the huge computation power needed to fully parallelize the
extension, it may be beneficial to use a very specialized unit (ASIC) just for
the computation of the cumulative distribution function.

Independent of the distribution, to not reduce the throughput of the fully
parallelized computation of the forward variables, a small pipeline of two
stages must be built, where in the first stage the transition probability
matrix is computed and in the second stage the forward variables.

The correction of the diagonal elements (cf. listing \ref{list:ext}) can be
maximal parallelized by using $N$ reduction trees to compute the sum of rows and
$N$ subtracters to correct the diagonal elements.

%-------------------------------------------------------------------------------
%===============================================================================
\section{Parallelization Options and Scalability}
\label{ch:analysis_all}

In the sections above, a lot of parallelization has been proposed. A maximal
parallelization can hardly be achieved due to the immense requirement of
recourses and is also not necessary because of dependencies. In a first step,
a theoretical analysis (in terms of complexity order) of the different
parallelization options and their combination is done. Then the results will be
discussed and a choice will be made. Finally some conclusions about a finer
degree of parallelization will be drawn in respect to the chosen architecture.
Table \ref{tab:summary_O} shows the pipelined architecture (cf. figure
\ref{fig:pipeline}), the parallel architecture in the case of maximal row
partitioning (cf. figure \ref{fig:matrix_partitioning}), the combination of
both architectures and in the last column the combination of both architectures
plus the reduction tree (cf figure \ref{fig:red_tree}). The basic and the
extended forward algorithm are both of the same complexity order. As only the
complexity order is considered, real computation times of different operations
as well as scaling operations can be ignored. Due to simplification, it is
assumed that $N=L$ and that by scaling the problem, $N$ and $L$ are both
changing in the same order.

\begin{table}
    \begin{center}
        \begin{tabular}{|l|*{4}{c|}}
            \hline
            Metric & Pipelined & Parallel & Both & Both \& Tree \\
            \hline
            \hline
            Computation Units
            & $O(N)$ & $O(N)$ & $O(N^2)$ & $O(N^3)$ \\
            \hline
            Memory Space
            & $O(N^2)$ & $O(N^2)$ & $O(N^2)$ & $O(N^2)$ \\
            \hline
            R/W Access
            & $O(1)$ & $O(N)$ & $O(N)$ & $O(N)$ \\
            \hline
            \hline
            Throughput
            & $\times N$ & $\times N$ & $\times N^2$ & $\times N^2\log N$ \\
            \hline
            Latency
            & $\times 1$ & $\times \frac{1}{N}$ & $\times \frac{1}{N}$
            & $\times \frac{1}{N\log N}$ \\
            \hline
        \end{tabular}
    \end{center}
    \caption{Comparison of architectures in terms of complexity for the forward
        algorithm}
    \label{tab:summary_O}
\end{table}

The two first columns show that the smaller latency of the parallel
architecture comes at the price of a larger memory interface (simultaneous
memory access is required). By combining both architectures (3rd column), the
throughput can be increased by factor $N$ at the cost of increasing the order
of computation units. Adding also the reduction tree, throughput and latency
are increased, resp. decreased by an additional factor of $\log N$. This comes
again at the cost of increasing the order of computation units further. While
the benefits are welcome, $N^3$ is simply to high to realistically implement
such a solution.  Already with a small $N$ a huge server farm or thousands of
FPGAs or GPUs would be necessary. Considering this, the reduction tree
parallelization method will not be used as it gives the fewest benefits
($O(LogN)$) for its cost ($O(N)$).  Computation units in an order of $N^2$ (2nd
column) is feasible for small $N$ by combining multiple devices. While this may
be acceptable for a very important computation where lots of people depend upon
(eg. weather forecast, Google queries, etc.) failure prediction hardly falls
into this category especially if failures of an embedded system are predicted.
Additionally, using multiple devices in order to scale the problem, implies
off-chip-communication and -memory. This will result in huge bottlenecks and
have a huge impact on the actual speedup. This leaves the first two columns to
compare for the application at hand (A combination of both methods can still be
considered, but not by using maximal parallelization).

Table \ref{tab:summary_D} now only compares the pipelined architecture with the
parallel architecture but with more detailed estimations of resource usage.
The comparison is done for the basic and the extended forward algorithm (for
explanations refer to the previous sections in this chapter). In case of the
extended forward algorithm, when the parallel architecture is used, there is
a choice to be made if the parallelization should be achieved by increasing the
memory usage or the number of computational units (hence the two columns).

\begin{table}
    \begin{center}
        \begin{tabular}{|c|l|*{3}{c|}}
        %\begin{tabular}{|c|l|*{7}{c|}}
            %\cline{3-9}
            \cline{3-5}
            \multicolumn{2}{c|}{}
            & Pipelined
            & \multicolumn{2}{c|}{Parallel}
            %& \multicolumn{2}{c|}{both}
            %& \multicolumn{2}{c|}{both \& tree}
            \\
            \hline
            \multirow{4}{*}{\rotatebox{90}{Basic}}
            & Computation Units
            & $L$
            & \multicolumn{2}{c|}{$N$}
            %& \multicolumn{2}{c|}{$N^2$}
            %& \multicolumn{2}{c|}{$N^3$}
            \\
            %\cline{2-9}
            \cline{2-5}
            & Memory Space
            & $2N^2+2LN+N$
            & \multicolumn{2}{c|}{$2N^2+3N$}
            %& \multicolumn{2}{c|}{$4N^2+N$}
            %& \multicolumn{2}{c|}{$4N^2+N$}
            \\
            %\cline{2-9}
            \cline{2-5}
            & Read Access
            & $4$
            & \multicolumn{2}{c|}{$N+2$}
            %& \multicolumn{2}{c|}{$N+3$}
            %& \multicolumn{2}{c|}{$2N+2$}
            \\
            %\cline{2-9}
            \cline{2-5}
            & Write Access
            & $1$
            & \multicolumn{2}{c|}{$N$}
            %& \multicolumn{2}{c|}{$N$}
            %& \multicolumn{2}{c|}{$N$}
            \\
            \hline
            \hline
            \multirow{4}{*}{\rotatebox{90}{Extended}}
            & Computation Units
            & $L+C$
            & $N+C$ & $(C+1)N$
            %& \multicolumn{2}{c|}{$(C+1)N^2$}
            %& \multicolumn{2}{c|}{$N^3+CN^2$}
            \\
            %\cline{2-9}
            \cline{2-5}
            & Memory Space
            & $(P+1)N^2+2LN+N$
            & $PL(1+N^2)+3N$ & $(P+1)N^2+3N$
            %& \multicolumn{2}{c|}{$2N^2+3N$}
            %& \multicolumn{2}{c|}{$2N^2+3N$}
            \\
            %\cline{2-9}
            \cline{2-5}
            & Read Access
            & $5$
            & \multicolumn{2}{c|}{$2N+2$}
            %& \multicolumn{2}{c|}{$2N+3$}
            %& \multicolumn{2}{c|}{$3N+2$}
            \\
            %\cline{2-9}
            \cline{2-5}
            & Write Access
            & $1$
            & \multicolumn{2}{c|}{$N$}
            %& \multicolumn{2}{c|}{$N$}
            %& \multicolumn{2}{c|}{$N$}
            \\
            \hline
        \end{tabular}
    \end{center}
    \caption{Pipelined versus parallel architecture for the basic and the
        extended forward algorithm (C: Number of computation units to compute
        one CDF, P: Number of parameters to compute one CDF)}
    \label{tab:summary_D}
\end{table}

The benefits of a parallel architecture over the pipelined are first and
foremost the reduced latency and in case of the basic algorithm also the lower
memory footprint. If the acceleration architecture of choice has a memory
interface that allows the required throughput, for the basic algorithm this
architecture should be chosen. In case of the extended algorithm this is only
possible if enough computational units are available (and the CDF computation
is not to complex) or if the on-chip memory is large enough to save transition
probabilities for later use.

For the basic algorithm, the pipelined architecture should only be chosen if
the memory interface becomes the bottleneck (for large N). Ideally
a combination of the parallel architecture and the pipeline should be chosen in
order to maximize the memory interface usage. By doing this, a smaller latency
is achieved by keeping the throughput high. Another reason to chose the
pipelined architecture would be a simple state transition model (cf. section
\ref{ch:analysis_simple}) which allows to save only the non-zero transition
probabilities in a list. From this optimisation the pipelined architecture
benefits on a much larger scale as less serial accumulations would be
necessary, while in the parallel architecture only the utilization of the
computational units would be reduced (less power consumption but no impact on
performance).

In case of the extended forward algorithm, it is almost always better to choose
the pipelined architecture: It uses either less memory or less computational
units, allows optimization in case of simple models and allows more time to
compute the transition probabilities. Parallelization is only possible for very
simple CDF computations and for a small $N$.

The scalability is in both cases limited, but more so with the parallel
architecture: If $N$ or $L$ becomes large such that
off-chip memory is necessary (already for very small $N$ or $L$ for CPUs and
GPUS, less so for more flexible architectures like FPGAs or ASICs) the memory
interface will be to small to handle memory access simultaneously and hence
become the bottleneck. The pipelined architecture does not have this drawback
but has a slightly higher memory footprint. If memory can be handled on-chip but
multiple chips are used to increase the number of recourses, both
architectures can be scaled easily in $L$ dimension (sequence length) but only
with difficulty in $N$ dimension (number of states) because the necessary
communication links (all components of the vector $\alpha_{k+1}$ always depends
on all components of the vector $\alpha_k$). The scaling in dimension $N$ of the
pipelined architecture only depends on the memory usage, while the parallel
architecture has a dependency of $N$ for the computation units as well as for
the memory.

%-------------------------------------------------------------------------------
%===============================================================================
\section{Selection of an Appropriate Accelerator Type}
\label{ch:analysis_choice}

\emph{\color{red}better title?}

For being able to choose an appropriate accelerator type, first a list with
different acceleration options is presented. For each type the most common
benefits and shortcomings are mentioned. At the end of the section a choice
will be made using the following list and the observations discussed in the
previous sections.

\begin{description}
    \item[CPU] \hfill \\
        The Central Processing Unit falls into the class of the general purpose
        processors and is (usually) of the type SISD\footnote{Flynn's Taxonomy:
        Single Instruction, Single Data Stream}. It is very flexible in terms
        of software interpretation (with the use of compilers) and allows to
        implement any kind of function in a fast, easy and maintainable
        fashion with no requirements in hardware knowledge.  Operations like
        division, exponential function and logarithm are available as well as
        the floating point number representation. All operations can be
        executed with very high precision and at high clock frequencies (up to
        3 GHz). CPUs come with the drawback of a high power consumption,
        limited parallelization options (small number of cores) and a fixed
        hardware architecture that causes big computation overheads
        (instruction pipeline, memory hierarchy, generalized computation
        units).
    \item[GPU] \hfill \\
        Like the CPU, the Graphics Processing Unit still falls into the class
        of general purpose processors due to the hardware abstraction layers.
        A GPU is composed of a lot of small but quite powerful streaming
        processors of the type SIMD\footnote{Flynn's Taxonomy: Single
        Instruction, Multiple Data Streams}. This processing power allows a lot
        of parallelization at a low price. As the CPU, also the GPU is very
        flexible in terms of software interpretation, requires however some
        hardware understanding in order to use the parallel power in an optimal
        way. Also a GPU allows to work with operations like division, exp and
        log functions and provides floating point number representation. GPUs
        can work with high precision and at high clock frequencies. The power
        consumption of a GPU is very high due to the high frequencies and the
        streaming processors. While the hardware abstraction layer provides
        flexibility in software and ease of use, it is the main reason for
        a computation overhead for basic operations. The fixed hardware
        architecture (especially the memory hierarchy) can prove to be
        a drawback in specific cases (eg. data usage provokes always cache
        misses).
    \item[DSP] \hfill \\
        The Digital Signal Processor is a specialized integrated circuit,
        largely used for digital signal processing. The key components of DSPs
        are optimized Multiply-Accumulate instructions, special SIMD operations
        and for DSP operation optimized memory architecture. They provide
        a hardware abstraction and are pretty easy to program. This causes
        some overhead as an instruction pipeline is necessary. The overhead is
        a lot smaller than in the case of CPUs because of specific instructions
        sets (this comes with a loss of flexibility). Fixed point as well as
        floating point devices exist with various precisions. A DSP provides
        a lot of specific computation power for a low price.
    \item[FPGA] \hfill \\
        The Field Programmable Gate Array is a customizable integrated circuit.
        It provides large recourses of logic gates as well as standard blocks
        such as RAM or highly optimized Multiply-Accumulate units. FPGAs
        provide high performance for very specific design optimized for one
        function. Once an FPGA is configured, as long as it keeps this
        configuration no other function will run on this device as it is the
        direct hardware representation of the function. This direct
        representation allows a very low overhead as operations are done
        directly in hardware without any instruction pipeline. FPGAs provide
        a high amount of hardware flexibility that is only toped by ASICs (see
        below). This flexibility comes at a medium price as FPGAs can be
        produced in big lots but are more difficult to produce than DSPs.
        A huge advantage is the possibility the build very big memory
        interfaces inside the chip and customize the memory architecture for
        the application at hand. The drawback of FPGAs is the increased
        development time necessary to implement a hardware solution of
        a function, the "low" clock frequency of up to 500MHz (this is low
        compared to CPUs or GPUs) and the absence of division, exponential and
        logarithm functions. All units are optimized for fixed point
        representation and floating point numbers must be implemented manually.
        Core generators and very powerful synthesis tools try to amend these
        drawbacks but still a very deep knowledge of hardware is necessary to
        successfully implement a design on FPGA.
    \item[ASIC] \hfill \\
        The Application Specific Integrated Circuit is, as the name already
        tells, an integrated circuit that has been designed for one (and only
        one) specific application. In comparison to the FPGA, an ASIC is built
        with fully customized elements and provides the full flexibility
        achievable with todays hardware knowledge. An ASIC has ideally no
        overhead as the hardware is a direct mapping of the function. This
        leads to very high performance at very low power consumption (as a rule
        of thumb a factor of 1000 can be assumed in either performance gain or
        power consumption decrease or a combination of both compared to CPUs).
        An ASICs is very expensive to produce. This includes the long
        development time and the production cost. Of course very deep hardware
        knowledge is necessary to create an ASIC of a specific function.
\end{description}

Comparing the accelerator types mentioned in the list above in a general
manner, one can conclude that the further down the list one goes, the better is
the performance - prower consumption ratio and the lower is the flexibility of
the device to accept general function descriptions (cf figure \ref{fig:hw}, eg.
a function description for a FPGA must be a lot more specific than one for
a GPU but the performance-power consumption ratio is a lot better in case of
the FPGA). For this work, one key point is high performance, because the
algorithm to implement is computation intensive and needs to be executed fast.
Another important point is the size of the system: the target application to
predict failures is an embedded system, where space is usually limited. Power
consumption may not be a main aspect, but it certainly needs to be considered
to not exceed requirements of the main system.

\begin{figure}
    \centering
    \input{./schema/hw.tex}
    \caption{Flexibilty versus Performance of Hardware Devices}
    \label{fig:hw}
\end{figure}

Due to the limited parallelization options or the huge power and space
requirements of CPUs, this accelerator type is not suitable for the application
in question. GPUs provide a lot of computation power at a low price but have
drawbacks in terms of architecture flexibility, space and power requirements.
GPUs are a far better option than CPUs but may not be ideal because of the
fixed memory interface. DSPs would be the device of choice if only the basic
forward algorithm is considered. Due to the high performance at a low cost of
DSPs, and the fact that the basic forward algorithm mainly uses
multiply-accumulate operations one or multiple floating point DSP devices could
be used to efficiently implement the algorithm. In case of the extended
algorithm a DSP device causes to big an overhead to compute the CDF and will be
the bottleneck of the design. For this reason a more flexible device must be
chosen.  The most flexible architecture is an ASIC. ASICs are very expensive in
terms of money and development time, two recourses that are not available for
this work. This leaves the FPGA: An FPGA combines the parallel power of DSPs
for Multiply-Accumulate operations but adds the possibility to design
a specific hardware architecture to compute the transition probabilities needed
for the extension. While the performance will not be at a level of an ASIC, the
specialized hardware will still outperform any other device because of the
customizable memory interface (even at the lower frequencies of FPGAs).

%===============================================================================
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Design and Implementation}
\label{ch:design}

Following the argumentation of the previous chapter, this chapter will describe
the design of the pipelined architecture of the extended forward algorithm on
a FPGA.

To design the accelerator, the top-down approach was applied: the algorithm is
broken down into blocks, where each of them is broken down further until the
basic functional blocs of the FPGA can be used for the implementation. The
implementation follows then the bottom-up approach where each sub-block is
implemented and tested. Completed blocks are grouped together to bigger blocks
until finally there is only one big block remaining, describing the complete
algorithm.

%-------------------------------------------------------------------------------
%===============================================================================
\section{Architecture}
\label{ch:design_arch}

The top architecture of the proposed algorithm is depicted in figure
\ref{fig:arch_top}. The non-failure as well as all the failure sequence
detection blocks denoted as "non-failure" and "failure type s" represent each
the complete forward algorithm as it was described in the previous chapter.
A system controller, denoted as "SYS CTRL" governs these blocks and the flash
(FLASH CRTL)and memory controllers (RAM CTRL) load data from persistent
respectively from volatile memory devices into the system. All the prediction
blocks calculate a sequence likelihood and feed it to the classifier denoted as
"classification". The classifier then decides if the present sequence leads to
a failure or not and outputs this result as a boolean value. In terms of
hardware, all the "failure" blocks are identical and can be run in parallel.
They only differ by the values that are fed into the internal memory of the
blocks.

\begin{figure}
    \centering
    \input{./schema/arch_top.tex}
    \caption{Top architecture of the failure prediction algorithm}
    \label{fig:arch_top}
\end{figure}

In the following the designing details of the failure blocks are presented.
The functionality of the pipelined architecture has already been described in
chapter \ref{ch:analysis_parallel}. The figure \ref{fig:pipeline} depicts the
basic schematics. The same architecture is shown in figure \ref{fig:arch_pipe}
with a simplified RTL (register transfer level) schematic. The main aspect of
the chosen design is to use the high performance multiply-accumulate units
available in modern FPGAs. These so called DSP-slices allow to perform, besides
other operation, fully pipelined multiply-accumulate operations at frequencies
of up to 450 MHz. The manual \cite{xilinx_DSP} describes the vast functional
possibilities of the DSP-slices available in the series 7 FPGAs of
Xilinx\footnote{http://www.xilinx.com}. As shown in figure \ref{fig:arch_pipe},
in each pipeline stage the transition probabilities (coming either from memory
in case of the basic forward algorithm or from a computational unit in the case
of the extended algorithm) are multiplied with the forward variables (read out
of the fifo queue) calculated in the previous stage and the results are
accumulated. This is done until $N$ components are accumulated, then the
emission probabilities are multiplied to the result and the first component of
the forward variable vector $\alpha_k$ of this stage is stored into a fifo
queue of the next pipeline stage. This iteration is repeated $N$ times until
all $N$ component of the $\alpha$ vector are computed. After each iteration, the
accumulator must be cleared. It takes three pipeline stages to pipeline the
multiply- accumulate operation and two stages for the multiply operation. It
takes $N+3$ cycles to accumulate all multiplication pairs ($\alpha_{k-1, j}
* tp_{i,j}$) and $2$ cycles to perform the final multiplication. This adds up
to $N^2+5)$ cycles to compute all components of the vector $\alpha_k$ (the
setup time needs only to be considered once if a well timed reset of the
accumulator is performed).

\begin{figure}
    \centering
    \input{./schema/arch_pipe.tex}
    \caption{Simplified RTL representation of the pipelined forward algorithm}
    \label{fig:arch_pipe}
\end{figure}

With this design, there are several things to note. A first concern focuses on
the second multiplier of the chain: It is used only once every $N+3$rd cycle
and computes either nonsense or is idle (if disabled) during the rest of the
time.  This is a very poor utilization of recourses and can be optimized by
reusing the first multiplier to do the second multiplication. Doing this
increases the necessary cycles to a minimum of $N*(N+3)+2$ cycles but cuts the
necessary multiplication units (DSP-slices) by half. Another point to note
concerns the FIFO queue. The queue is supposed to store the arriving components
of the vector $\alpha_{k-1}$ and use them to compute the next vector
$\alpha_k$.  However, each component of the vector $\alpha_{k-1}$ is used $N$
times, as every component of the new vector $\alpha_k$ depends on every
component of the previous vector $\alpha_{k-1}$. Therefore it is not possible
to use only one FIFO queue. A solution to this problem is to use two queues,
one to store the arriving new values needed for the next iteration and one to
read the values that have been stored last iteration. At the end of the
computation of all components of the vector $\alpha$, the queues are switched.
While a queue is in read state, it operates as a circular buffer by storing
back the value it has just read. Another solution is to use addressable memory
blocks instead of FIFOs and use a simple counter to increasingly address the
required value to read or to store a new vale. While the solution with the
FIFOs is easier to control, the memory block solution offers another big
advantage: if a simple model for the state transitions is used (as discussed in
chapter \ref{ch:analysis_simple}), a lot of components of the transition
probability matrix will be zero. It doesn't make sense to compute the product
of zero with a component of the $\alpha$ vector as the result will always be
zero and won't impact the accumulation. Using an addressable memory block, the
$\alpha$ vector components corresponding to a zero in the transition
probability matrix could easily be skipped while with the FIFOs, this would not
be possible. Choosing an appropriate method to store the sparse matrix in
memory, the computation of the new address to read the next $\alpha$ vector
component corresponding to a non zero transition probability can be done very
fast and with only few recourses (see chapter \ref{ch:design_mem} for more
information).

\begin{figure}
    \centering
    \input{./schema/arch_step.tex}
    \caption{RTL implementation of a pipeline stage with dual memory queue and
        reusage of components}
    \label{fig:arch_step}
\end{figure}

Figure \ref{fig:arch_step} shows the detailed design of one pipeline stage, if
the multiplication unit (DSP-slice) is reused and if to memory blocks are used,
alternating one to write new data and one to read data stored from the last
$\alpha$ vector computation. The control signal are not labeled here for reasons
of readability and will be explained in detail in chapter \ref{ch:design_ctrl}.

\emph{\color{red}describe the design of the extension}

%-------------------------------------------------------------------------------
%===============================================================================
\section{Operand Scaling and Precision}

The DSP-Slices used to for multiply and accumulate operations are designed for
fixed point data representation, hence thy do not support floating point
operations. This poses no problem if an operand width can be chosen that allows
to represent the complete spectrum of all possible values an operand can take
(with respect to a certain precision). The continuous multiplication of
probabilities, as it is the case with the algorithm at hand, lead to very small
values very fast, depending on the sequence length $L$. This is the reason why
a fixed point representation without scaling cannot be used. This can be
circumvented by using floating point numbers with a very large exponent to
prevent an underflow. In this case, the DSP-Slices need to be extended in order
to support floating points. In the following this option is discussed and then
compared to a fixed point solution with scaling.

In order to multiply two floating point numbers, the mantissas of both numbers
are multiplied and the exponents are added (the sign can be ignored, as
probabilities are always positive). For the multiplication of the mantissas the
DSP-Slices can be used. In parallel to this operation, an external adder can
add up the exponents. For being able to add floating point numbers (which
should be done with the accumulator of the DSP-Slice) the exponents of the
numbers must be equal. To achieve that, the difference of the two exponents is
calculated and then the mantissa of the number with the lower exponent must be
shifted by this difference (and possibly rounded/truncated). This process is
called normalizing. After the normalization, the mantissas can be added and
finally the resulting value needs to be normalized again.

The work \cite{FPL13_Brosser} presents a solution for a floating point
operations, including multiplication and addition, in single
precision\footnote{IEEE 754-2008} using only one DSP-Slice. For being able to
omit the scaling completely, the exponent would need to be larger: assuming
that the alphabet contains $M=1000$ elements and using the estimation of
equation \ref{eq:scaling_estimation}, a sequence length of $L>13$ would already
exceed the capabilities of single precision data representation. The drawback
of this solution is the dependency of the exponent on the sequence length $L$
and the non-standardized representation. The proposed solution would needed to
be extended by a parametrizable exponent size. Another issue in this work is
the absence of a optimized and fully pipelined multiply-accumulate operation.
In order to do multiplication followed by accumulation with the same DSP-Slice,
one operation must first finish completely before the other can begin. This has
a huge impact on the latency as well as on throughput: Using the latencies
presented in the work, 22 cycles are necessary for multiplication and 25 cycles
for addition. Using memory to store the intermediate results the operations
could be pipelined, but without additional memory no pipelining is possible.
A more optimized solution for the algorithm at hand can surely be found but
a more profound analysis of floating point operations on FPGA must be performed
to gain more insight. This is however beyond of the scope of this work and the
present facts are enough to first consider a fixed point solution with scaling
before heading deeper into floating points.

The documents \cite{smith97, ti04} discuss the choice of fixed point versus
floating point representation with respect to DSP devices. This is also
applicable for FPGAs.

\emph{\color{red} use the argumentation of those documents}

The DSP-Slices allow fixed point multiply-accumulate operations with a operands
of bit width 18 and one of bit width 25. Internally the device works with a bit
width of 48 bits, which allows a lossless multiplication (43 bits) and an
accumulation with 5 bits margin for overflow. Due to the properties expressed in
equations \ref{eq:scaling_sum}, these fife additional bits are not necessary in
the present case. After the MACC operation, the second multiplication takes
a gain operands of with 18 bits resp 25 bits as input. Therefore, the result of
the MACC needs to be truncated. In order to not loose to much information, at
this step scaling must be introduced.

As already discussed in chapter \ref{ch:analysis_scaling} the scaling method
proposed by the reference work must be avoided in this implementation due to
the additional necessary operations. Instead, a scaling in base of 2 can be
used (this corresponds to a shift): after the computation of a component of the
initial $\alpha$ vector, a lead zero counter (LZC) unit is introduced. This
unit is purely combinational and counts the leading zeros of a operand. The
minimal count of all components in the $\alpha$ vector is then forwarded into
the next pipeline stage where it is used to to shift the result of the MACC
operation for LZC positions to the left (this corresponds to a multiplication
of $2^{LZC}$). Also in this stage (and every following) a LZC unit is added in
the same way as described above. The LZC results of each stage are forwarded
and used to shift the results and accumulated. The minimum LZC of all elements
of the $\alpha$ vector has to be found because all operands of one stage need
to be scaled by the same value. It must be the minimum value in order to
prevent a overflow in the shifting operation. It is important to use the same
value for all operants to being able to accumulate the operands without
normalization in the next pipeline stage.

Simultaneously to the resulting likelihood, also a scaling factor is provided.
This scaling factor can then be used in the classification step to normalize
all the involved likelihood values and decide if a failure is predicted for the
current event sequence. The scaling method is presented in figure
\ref{fig:arch_pipe_scale}.

\begin{figure}
    \centering
    \input{./schema/arch_pipe_scale.tex}
    \caption{Simplified RTL representation of the pipelined forward algorithm
        with scaling using shifters and leading zero counters}
    \label{fig:arch_pipe_scale}
\end{figure}

To perform the scaling, also a DSP-Slice can be used by simply multiplying the
operand with the factor $2^{LZC}$. The whole range of internal 43 bits of the
MACC operation need to be scaled which can be achieved by two cycles of one
DSP-Slice. First the lower 18 bits ($op_{low}$) of the 43 internal bits ($op$)
are introduced into the DSP-Slice and multiplied by the factor $2^{LZC}$.
A precomputed lookup table is used to select the factor corresponding to
$2^{LZC}$ using LZC as input. In next cycle, the upper 25 bits ($op_{up}$) of
$op$ are selected and introduced into the DSP-Slice. $op_{up}$ is shifted by
a constant amount of 17 bits (this can be done internally by the DSP-Slice) and
then also multiplied by the factor $2^{LZC}$. The accumulator then adds the two
shifted operands $op_{low}$ and $op_{up}$ together which results in a shifted
value of $op$ by LZC bits to the right. All these operation can be done by
using only one DSP-Slice. Figure \ref{fig:arch_step_scale} shows the necessary
schematics. Also the simplified representation of the memory queue has been
replaced in this schematic.

\begin{figure}
    \centering
    \input{./schema/arch_step_scale.tex}
    \caption{RTL implementation of a pipeline stage including scaling}
    \label{fig:arch_step_scale}
\end{figure}

To select the minimal LZC value, two registers and a comparator are used. The
first register stores the new LZC value and the second register holds the output
LZC value. The output register is only updated with the new value if the new
value is smaller than the one already stored in the output register.

To further increase the precision, the stored values $\pi$, $B$ and $TP$ can be
preprocessed and scaled. Also here, the element with the lowest number of
leading zeros decides the scaling factor.

\emph{\color{red}calculation of quantization error, conclusion that fixed points
are fine}

%-------------------------------------------------------------------------------
%===============================================================================
\section{Memory Architecture and Management}
\label{ch:design_mem}

internal block ram of the FPGA for $\pi$, $TP$ and $B$. In case of the extension,
instead of $TP$ the cdf parameters are stored.

$\pi$ and $TP$/cdf parameter values can be stored once and for all (they never
change)

$B$ changes depending on the arriving event. There are usually to much different
events ($M=1000$) to store all inside the FPGA. A two staged pipeline is
necessary to keep the throughput at a maximum level. In case of the extension
this top level pipeline already exist. In the first stage of the pipeline,
a memory controller reads from an external memory the $B$ values corresponding
to the actual event and stores them into the internal buffer. This buffer is of
the type FIFO. This needs to be done for every active prediction model (cf.
figure \ref{fig:arch_top})

The events do not arrive in a regular interval. They may arrive very fast one
after each other and there my be time intervals where no event arrives. No hard
real time constraints for the prediction system exist: There is no impact on
the prediction quality, if the latency from the time an event arrives until the
likelihood is computed differs depending on the load of the system. This can be
used to even out the event stream by introducing a FIFO queue where the events
are stored until the system is able to compute the corresponding likelihood.
This buffer needs to be designed in order to be able to collect events when the
accelerator is used to full capacity and events are arriving faster than the
accelerator can handle. Ideally, the buffer should never be empty and never
full. A buffer that is always empty, indicates that the performance of the
accelerator is is to high. The frequency of the accelerator can be lowered in
order to save more energy. If the buffer gets filled up, it becomes
problematic, because new events cannot be stored and are lost. If the average
frequency of the arriving events is lower than the maximal throughput the
buffer is designed to small.  If the frequency is higher, the performance of
the accelerator is insufficient.

As already mentioned in chapter \ref{ch:design_arch}, intermediate vectors 
$\alpha_k$ need to be stored in memory, as each value is used multiple times to
compute the next vector $\alpha_{k+1}$. FPGAs provide internal memory blocks
that can be accessed in one clock cycle. These memory blocks limit the maximal
possible values of $N$ and $L$ to be implemented on one FPGA. $L$ is also
limited by the available MACC units. Table \ref{tab:summary_D} shows the
necessary memory in function of $N$ and $L$.

\emph{\color{red}how to store sparse matrices (wrt chapter
\ref{ch:design_arch})}

%-------------------------------------------------------------------------------
%===============================================================================
\section{Controlling the Pipeline}
\label{ch:design_ctrl}

\begin{figure}
    \centering
    \input{./schema/arch_sm.tex}
    \caption{Mealey State Machine to control the pipeline}
    \label{fig:arch_sm}
\end{figure}

\emph{\color{red} in table \ref{tab:ctrl} the signals flush\_ps and load\_out
are only set in the first iteration. add a footnote?}

%\begin{table}
%    \begin{center}
%    \begin{tabular}{|l|c|c|c|c|c|c|c|}
%    \hline
%    signal            & init & select & macc & conciliate & mul & store & flush \\
%    \hline
%    \hline
%    shift\_alpha\_in  & 0    & 0      & 1    & 0          & 0   & 0     & 0     \\
%    shift\_alpha\_out & 0    & 0      & 0    & 0          & 0   & 1     & 0     \\
%    conciliate        & 0    & 0      & 0    & 1          & 0   & 0     & 0     \\
%    enable\_init      & 0    & 0      & 0    & 0          & 1   & 0     & 0     \\
%    enable\_step      & 0    & 0      & 1    & 1          & 1   & 0     & 0     \\
%    enable\_final     & 0    & 1      & 0    & 0          & 0   & 0     & 0     \\
%    flush             & 0    & 0      & 0    & 0          & 0   & 0     & 1     \\
%    \hline
%    \end{tabular}
%    \end{center}
%    \caption{control signals}
%    \label{tab:ctrl}
%\end{table}

\begin{table}
    \footnotesize
    \begin{center}
    \begin{tabular}{|c|l|*{10}{c|}}
    \hline
    type & signal                      & init & select & macc & conciliate & shift1 & shift2 & mul & store & flush & input       \\
    \hline
    \hline
         & pi\_we                      &      &        &      &            &        &        &      &      &       & input       \\
         & tp\_we                      &      &        &      &            &        &        &      &      &       & input       \\
         & b\_we                       &      &        &      &            &        &        &      &      &       & input       \\
         & data\_ready                 &      &        &      &            &        &        &      &      &       & input       \\
    1    & enable\_count               & 0    & 0      & 0    & 0          & 0      & 0      & 0    & 1    & 0     & pi\_we      \\
    2    & enable\_ctrl                & 0    & 0      & 0    & 0          & 0      & 0      & 0    & 0    & 0     & data\_ready \\
    3    & enable\_init                & 0    & 0      & 0    & 0          & 0      & 0      & 1    & 0    & 0     & data\_ready \\
    3    & enable\_init\_mul           & 0    & 0      & 0    & 0          & 0      & 0      & 1    & 0    & 0     & data\_ready \\
    4    & enable\_step                & 0    & 0      & 1    & 1          & 1      & 1      & 1    & 0    & 0     & data\_ready \\
    4    & enable\_step\_macc          & 0    & 0      & 1    & 1          & 1      & 1      & 1    & 0    & 0     & data\_ready \\
    5    & enable\_final               & 0    & 1      & 0    & 0          & 0      & 0      & 0    & 0    & 0     & data\_ready \\
    4    & laod\_op2                   & 0    & 0      & 1    & 1          & 1      & 1      & 1    & 0    & 0     & data\_ready \\
    6    & load\_step\_alpha           & 0    & 0      & 1    & 0          & 0      & 0      & 0    & 0    & 0     &             \\
    7    & load\_final\_alpha          & 0    & 0      & 0    & 0          & 0      & 0      & 0    & 1    & 0     &             \\
    8    & load\_scale\_new            & 0    & 1(d)   & 0    & 0          & 0      & 0      & 0    & 0    & 0     &             \\
    9    & load\_scale\_acc            & 0    & 1      & 0    & 0          & o      & 0      & 0    & 0    & 0     &             \\
    7    & store\_init\_scale\_new     & 0    & 0      & 0    & 0          & 0      & 0      & 0    & 1    & 0     &             \\
    23   & store\_init\_scale\_small   &      & 1(d)   &      &            &        &        &      &      &       & or internal \\
    9    & store\_init\_scale\_ok      & 0    & 1      & 0    & 0          & 0      & 0      & 0    & 0    & 0     &             \\
    7    & store\_step\_alpha          & 0    & 0      & 0    & 0          & 0      & 0      & 0    & 1    & 0     &             \\
    7    & store\_step\_scale\_new     & 0    & 0      & 0    & 0          & 0      & 0      & 0    & 1    & 0     &             \\
    23   & store\_step\_scale\_small   &      & 1(d)   &      &            &        &        &      &      &       & or internal \\
    9    & store\_step\_scale\_ok      & 0    & 1      & 0    & 0          & 0      & 0      & 0    & 0    & 0     &             \\
    5    & store\_final\_ps            & 0    & 1      & 0    & 0          & 0      & 0      & 0    & 0    & 0     & data\_ready \\
    10   & store\_final\_ps\_delayed   & 0    & 1(d)   & 0    & 0          & 0      & 0      & 0    & 0    & 0     & data\_ready \\
    10   & store\_final\_scale         & 0    & 1(d)   & 0    & 0          & 0      & 0      & 0    & 0    & 0     & data\_ready \\
    11   & shift\_step\_acc            & 0    & 0      & 0    & 1(6)       & 0      & 0      & 0    & 0    & 0     &             \\
    12   & sel\_mux2\_op2              & 0    & 0      & 0    & 0          & 0      & 0      & 1    & 0    & 0     &             \\
    13   & sel\_step\_read\_fifo       &      & (*)    &      &            &        &        &      &      &       & special     \\
    12   & sel\_step\_op1              & 0    & 0      & 0    & 0          & 0      & 0      & 1    & 0    & 0     &             \\
    14   &                             & 0    & 0      & 0    & 1          & 1      & 0      & 0    & 0    & 0     &             \\
    15   &                             & 0    & 0      & 0    & 1          & 0      & 1      & 0    & 0    & 0     &             \\
    16   & sel\_step\_op2              & 0    & 0      & 0    & 1(5)       & 1      & 1      & 0    & 0    & 0     &             \\
    17   & flush\_init                 & 0    & 0      & 0    & 0          & 0      & 0      & 0    & 0    & 1     &             \\
    17   & flush\_step\_macc           & 0    & 0      & 0    & 0          & 0      & 0      & 0    & 0    & 1     &             \\
    18   & flush\_step\_acc            & 0    & 0      & 0    & 0          & 0      & 1      & 0    & 0    & 0     &             \\
    9    & flush\_step\_fifo           & 0    & 1      & 0    & 0          & 0      & 0      & 0    & 0    & 0     &             \\
         & reset\_n                    & 0    & 0      & 0    & 0          & 0      & 0      & 0    & 0    & 0     & input       \\
    19   & reset\_count\_n             & 0    & 0      & 0    & 0          & 0      & 0      & 0    & 0    & 0     & reset\_n    \\
    19   & reset\_op2\_n               & 0    & 0      & 0    & 0          & 0      & 0      & 0    & 0    & 0     & reset\_n    \\
    19   & reset\_ctrl\_n              & 0    & 0      & 0    & 0          & 0      & 0      & 0    & 0    & 0     & reset\_n    \\
    19   & reset\_init\_n              & 0    & 0      & 0    & 0          & 0      & 0      & 0    & 0    & 0     & reset\_n    \\
    20   & reset\_init\_mul\_n         & 0    & 0      & 0    & 0          & 0      & 0      & 0    & 0    & 1(n)  & reset\_n    \\
    19   & reset\_step\_n              & 0    & 0      & 0    & 0          & 0      & 0      & 0    & 0    & 0     & reset\_n    \\
    20   & reset\_step\_macc\_n        & 0    & 0      & 0    & 0          & 0      & 0      & 0    & 0    & 1(n)  & reset\_n    \\
    21   & reset\_step\_fifo0          & 0    & 1(*(n))& 0    & 0          & 0      & 0      & 0    & 0    & 0     & reset       \\
    22   & reset\_step\_fifo1          & 0    & 1(*)   & 0    & 0          & 0      & 0      & 0    & 0    & 0     & reset       \\
    19   & reset\_step\_scale\_new\_n  & 0    & 0      & 0    & 0          & 0      & 0      & 0    & 0    & 0     & reset\_n    \\
    19   & reset\_step\_scale\_small\_n& 0    & 0      & 0    & 0          & 0      & 0      & 0    & 0    & 0     & reset\_n    \\
    19   & reset\_step\_scale\_ok\_n   & 0    & 0      & 0    & 0          & 0      & 0      & 0    & 0    & 0     & reset\_n    \\
    19   & reset\_init\_scale\_new\_n  & 0    & 0      & 0    & 0          & 0      & 0      & 0    & 0    & 0     & reset\_n    \\
    19   & reset\_init\_scale\_small\_n& 0    & 0      & 0    & 0          & 0      & 0      & 0    & 0    & 0     & reset\_n    \\
    19   & reset\_init\_scale\_ok\_n   & 0    & 0      & 0    & 0          & 0      & 0      & 0    & 0    & 0     & reset\_n    \\
    19   & reset\_final\_n             & 0    & 0      & 0    & 0          & 0      & 0      & 0    & 0    & 0     & reset\_n    \\
    \hline
    \end{tabular}
    \end{center}
    \caption{detailed control signals}
    \label{tab:ctrl_detailed}
\end{table}

%-------------------------------------------------------------------------------
%===============================================================================
\section{Balancing the Pipeline Stages}

%-------------------------------------------------------------------------------
%===============================================================================
\section{Implementation and Testing}

\begin{itemize}
    \item generic design (change n, l and bit widths in param\_pkg)
    \item if bit widths of op1 is changed, fifo\_512x25 must be regenerated
\end{itemize}

%===============================================================================
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Results}
\label{ch:results}

\begin{itemize}
    \item nexys4 board with artix-7 fpga
    \item limited recourses -> proof of concept
    \item bord hardware for testing
\end{itemize}

%-------------------------------------------------------------------------------
%===============================================================================
\section{Speedup}

%-------------------------------------------------------------------------------
%===============================================================================
\section{Accuracy}

%===============================================================================
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Conclusion}
\label{ch:conc}

%-------------------------------------------------------------------------------
%===============================================================================
\section{Main Contribution}
\label{ch:conc_ach}

%-------------------------------------------------------------------------------
%===============================================================================
\section{Future Work}
\label{ch:conc_work}

\nocite{*}

\appendix %optional, use only if you have an appendix

%===============================================================================
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{some material}
%\section{it's over\dots}

\backmatter

%\chapter{glossary} %optional

%\bibliographystyle{alpha}
%\bibliographystyle{dcu}
%\bibliographystyle{plainnat}
%\bibliographystyle{plain}
%\bibliographystyle{abbrvnat}
\bibliographystyle{siam}
%\bibliographystyle{ieeetr}
\bibliography{biblio}

%\cleardoublepage
%\theindex %optional, use only if you have an index, must use
	  %\makeindex in the preamble

\end{document}
