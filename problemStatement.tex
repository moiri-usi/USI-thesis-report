%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Simple Sectioned Essay Template
% LaTeX Template
%
% This template has been downloaded from:
% http://www.latextemplates.com
%
% Note:
% The \lipsum[#] commands throughout this template generate dummy text
% to fill the template out. These commands should all be removed when 
% writing essay content.
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%----------------------------------------------------------------------------------------
%	PACKAGES AND OTHER DOCUMENT CONFIGURATIONS
%----------------------------------------------------------------------------------------

\documentclass[12pt]{article} % Default font size is 12pt, it can be changed here

\usepackage[utf8]{inputenc}

%\usepackage[square,sort,numbers]{natbib}

\usepackage{geometry} % Required to change the page size to A4
\geometry{a4paper} % Set the page size to be A4 as opposed to the default US Letter

\usepackage{graphicx} % Required for including pictures

\usepackage{float} % Allows putting an [H] in \begin{figure} to specify the exact location of the figure
\usepackage{wrapfig} % Allows in-line images such as the example fish picture

\usepackage{lipsum} % Used for inserting dummy 'Lorem ipsum' text into the template

\usepackage{mathtools}

\linespread{1.2} % Line spacing

%\setlength\parindent{0pt} % Uncomment to remove all indentation from paragraphs

\graphicspath{{Pictures/}} % Specifies the directory where pictures are stored

\begin{document}

%----------------------------------------------------------------------------------------
%	TITLE PAGE
%----------------------------------------------------------------------------------------

\begin{titlepage}

\newcommand{\HRule}{\rule{\linewidth}{0.5mm}} % Defines a new command for the horizontal lines, change thickness here

\center % Center everything on the page

\textsc{\LARGE Universit√† della Svizzera Italiana}\\[1.5cm] % Name of your university/college
\textsc{\Large Master Thesis}\\[0.5cm] % Major heading such as course name
\textsc{\large Accelerator for Event-based Failure Prediction}\\[0.5cm] % Minor heading such as course title

\HRule \\[0.4cm]
{ \huge \bfseries Problem Statement}\\[0.4cm] % Title of your document
\HRule \\[1.5cm]

\begin{minipage}{0.4\textwidth}
\begin{flushleft} \large
\emph{Author:}\\
Simon \textsc{Maurer} % Your name
\end{flushleft}
\end{minipage}
~
\begin{minipage}{0.4\textwidth}
\begin{flushright} \large
\emph{Supervisor:} \\
Prof. Miroslaw \textsc{Malek} % Supervisor's Name
\end{flushright}
\end{minipage}\\[4cm]

{\large \today}\\[3cm] % Date, change the \today to a set date if you want to be precise

%\includegraphics{Logo}\\[1cm] % Include a department/university logo - this will require the graphicx package

\vfill % Fill the rest of the page with whitespace

\end{titlepage}

%----------------------------------------------------------------------------------------
%	TABLE OF CONTENTS
%----------------------------------------------------------------------------------------

\tableofcontents % Include a table of contents

\newpage % Begins the essay on a new page instead of on the same page as the table of contents 

%----------------------------------------------------------------------------------------
%	INTRODUCTION
%----------------------------------------------------------------------------------------

\section{Introduction} % Major section

This document provides an outline of the planned work for the master thesis
"Accelerator for Event-based Failure Prediction". The document addresses the
problem considered by the thesis, a state of the art outline as well as
important questions and hypothesis that arise or are needed for the thesis.
Further the document provides a list of necessary theory bases and introduces
the design and experimenting methods applied in the thesis. Finally, the
document gives an overview of necessary material in terms of literature and
equipment, shows the preliminary work progress and provides a broad time
schedule.

%----------------------------------------------------------------------------------------
%----------------------------------------------------------------------------------------

\section{Problem and Research Area} % Major section

In today's live it becomes increasingly important, that computer systems are
dependable. The reason being, that computer systems are used more and more in
areas where the failure of such a system can lead to catastrophic events.
Banking, public transportation and medical engineering are only a few examples
of areas employing large and extremely complex systems. The increasing
complexity of computer systems has a direct impact on their maintainability and
testability. It is simply impossible to guarantee that a piece of software comes
without any faults. On top of that, the same problematic arises with the
hardware components which also may contain faulty parts but also get
increasingly prone to failures due to decay of material.

In the event of a system failure it is of course desirable to fix the system as
soon as possible in order to minimize the downtime of the system (maximize the
availability). This can be accomplished by using different types of recovery
techniques, e.g. Check-pointing (create checkpoints to roll back/forward),
system replication (switch to a redundant system), fail over (reboot). All these
techniques require a certain amount of time to complete the recovery process,
time that is very expensive. In order to minimize this time, techniques have
been developed to anticipate upcoming failures. Such a technique is described in
\cite{salfner08}.

The work presents a new algorithm to predict failures and compares the results
with other techniques. The accuracy of the presented algorithm to predict
failures proves to be better compared to the other techniques, has however the
drawback of increased complexity and hence increased computation time. It is
very important to keep the computation overhead very low in order to maximize
the time between the prediction of a failure and the actual event of the
failure. One way to decrease the computation time is to design a hardware
accelerator for the prediction algorithm. The design of such an accelerator is
outlined in this document.

The presented algorithm uses system information, e.g. log entries, to predict
failures during runtime (online). To be able to interpret the log files, it is
important that they are structured in a standardized format. Further it is
necessary input real log files to the algorithm in order to verify its
performance (in terms of precision and speed). Unfortunately it is not possible
to use the same data already used in the reference work, because of
confidentiality issues. For this reason, new test data have to be generated by
respecting the data properties described in the reference work as well as
respecting a log representation standard.

%----------------------------------------------------------------------------------------
%----------------------------------------------------------------------------------------

\section{State of the Art Outline} % Major section

This section provides a brief outline of the state of the art in the different
fields of research that are relevant for the thesis. This includes a small
overview of failure prediction methods, existing solutions to accelerate
failure prediction algorithms and concepts of log standardization.

%------------------------------------------------

\subsection{Failure Prediction} % Sub-section

A very detailed overview of failure prediction methods is given in
\cite{ACM10_Salfner}. The techniques used as comparison in the main reference
\cite{salfner08} as well as the described technique are listed below.

\begin{itemize}
    \item Event-based Failure Prediction \cite{salfner08} 
    \item Dispersion Frame Technique (DFT) \cite{lin88,IEEE90_lin}
    \item Eventset \cite{ICDM02_Vilalta}
    \item SVD-SVM \cite{domeniconi02}
\end{itemize}

Maybe some words about the counter ideas (SEER: a lightweight online failure
prediction approach)

Discuss the arguments of Felix
\begin{description}
    \item[Too many parameters to be identified, estimated and set] \hfill \\
        Considering an embedded system, this is usually not a problem because
        the parameters are defined during the design phase and will never be
        changed.
    \item[Limited performance scalability] \hfill \\
        \dots to be analyzed \dots
    \item[Industry trends towards cloud, not single node prediction] \hfill \\
        In embedded systems it will still be beneficial to predict failures of
        single nodes.
    \item 
\end{description}


%------------------------------------------------

\subsection{Accelerator} % Sub-section

The following list presents designs to accelerate algorithms related to machine
learning and the computation of hidden Markov models:
\begin{itemize}
    \item \cite{ASAP12_Azhar} presents an architecture for a lightweight Viterbi
        accelerator designed for a embedded processor datapath.
    \item \cite{FCCM09_Cadambi} describes a FPGA based accelerator for the
        SVM-SMO (support vector machine - sequential minimal optimization)
        algorithm used in the domain of machine learning.
    \item \cite{IPDPS07_Jacob,ICS06_Maddimsetty,IPDPS07_Oliver} describes a FPGA
        based accelerator for protein sequence HHM search. The Viterbi algorithm
        is used.
    \item \cite{IPDPS09_Walters} describes i.a. an approach to accelerate the
        Viterbi algorithm from the HMMER library using GPUs.
\end{itemize}

%------------------------------------------------

\subsection{Log Standardization} % Sub-section

\cite{IPDPS04_Salfner} describes an approach on how to standardize logs. Still
missing are sources for:
\begin{itemize}
    \item data generation in order to verify an algorithm
    \item metrics to measure log quality
    \item more on log standardization
\end{itemize}

%----------------------------------------------------------------------------------------
%----------------------------------------------------------------------------------------

\section{Questions and Hypothesis} % Major section

The following list is intended to give an overview of open questions:
\begin{itemize}
    \item how to parallelize the algorithm
    \item what kind of accelerator (FPGA, GPU, just use multiple cores)
    \item how to generate test data to verify the accelerator
    \item \dots
\end{itemize}

Specific questions concerning the offline algorithm:
\begin{itemize}
    \item what type of kernels (for transition durations $ D(t) $) and how many
    \item gradient of kernel partially derived by parameters: also sum over k
    \item how to compute the step size $ \eta $ (line search: $ \eta_k
        = h(\eta) $, choice of $ h(\eta) $)
    \item choice of bounds to stop iteration (step size, sequence likelihood)
    \item \dots
\end{itemize}

Ideas on how to accelerate the online part of the algorith
\begin{itemize}
    \item use high speed multiplier-accumulator (MAC) devices on a FPGA
    \item use MACs only on integer numbers and compute FP later manually
    \item minimize division (compute scaling factor once and then multiply)
    \item if precision allows use pipelining to precompute the factor
        $ b(j, o[k]) * v(i, j, k) $
    \item \dots
\end{itemize}

Possible optimizations of the algorithm:
\begin{itemize}
    \item use a regularization term in the cost function to prevent overfitting
    \item incorporate the offline part of the algorithm into the online part in
        order to deal with model aging
    \item \dots
\end{itemize}

The accelerator will be designed by taking the following hypotheses into account:
\begin{itemize}
    \item Algorithm of Felix has been verified in his work
    \item the system using the accelerator provides standardized log events
    \item only the online part of the algorithm needs to be accelerated
    \item \dots
\end{itemize}

%----------------------------------------------------------------------------------------
%----------------------------------------------------------------------------------------

\section{Theory Base} % Major section

%------------------------------------------------

\subsection{Taxonomy of Dependable Computing} % Sub-section
\begin{itemize} % Numbered list example
    \item Fault-Error-Failure chain \cite{IEEE04_Avizienis}
    \item Precision and Recall \cite{rijsbergen79}
    \item F-Measure \cite{DARPA99_Makhoul}
\end{itemize} 

%------------------------------------------------

\subsection{Algorithm Description} % Sub-section

This section provides an brief overview of the computational steps done by the
proposed algorithm. The following description should provide a complete
blueprint of the algorithm, that allows to implement it, but without any
explications or proofs related to the formulation. To be able to understand the
formal expression of the algorithm, first a definition of the used parameters
is provided.
\begin{itemize}
    \item N: number of states
    \item M: number of observation symbols
    \item L: observation sequence length
    \item R: number of cumulative probability distributions (kernels)
\end{itemize}
The delay of the event at time $ t_k $ with respect to the event at time
$ t_{k-1} $ is described as
\begin{equation}
    d_k = t_k-t_{k-1}
\end{equation}
One part of the algorithm is the model training. The features to be trained are
the following:
\begin{itemize}
    \item $ \pi_i $, forming the initial state probability vector
        $ \boldsymbol{\pi} $ of size $ N $
    \item $ b_i(o_j) $, forming the emission probability matrix $ B $ of size
        $ N \times M $
    \item $ p_{ij} $, forming the matrix of limiting transmission probabilities
        $ P $ of size $ N \times N $
    \item $ \omega_{ij, r} $, the weights of the kernel $ r $
    \item $ \theta_{ij, r} $, the parameters of the kernel $ r $
\end{itemize}
For simplification reasons, in a first step only one kernel is used. Due to 
this, the kernel weights can be ignored. Choosing the gaussian distribution
results in the kernel parameters $ \mu_{ij} $ forming $ \Theta_{\mu} $ and
$ \sigma_{ij} $ forming $ \Theta_\sigma $.
\begin{equation}
    \kappa_{ij, gauss}(d_k | \mu_{ij}, \sigma_{ij}) = 
        \frac{1}{\sigma_{ij} \sqrt{2 \pi}}
        \exp(-0.5 \frac{(d_k - \mu_{ij})^2}{\sigma_{ij}^2})
\end{equation}
The training of the model is accomplished by the following steps.
\begin{enumerate}
    \item initialize $ \boldsymbol{\pi} $, $ B $, $ P $, $ \mu $ and $ \sigma $
\end{enumerate}
\begin{equation}
    d_{ij}(d_k) = \sum_{r=1}^{R} \omega_{ij,r}\kappa_{ij,r}(d_k|\theta_{ij, r})
\end{equation}
\begin{equation}
    v_{ij}(d_k) = \left\{
        \begin{array}{l l}
            p_{ij} d_{ij}(d_k)
                & \quad \text{if $j \neq i$}\\
            1 - \sum\limits_{\substack{h=1 \\ h \neq i}}^{N} p_{ih} d_{ih}(d_k)
                & \quad \text{if $j = i$}
        \end{array} \right.
\end{equation}
%\begin{equation}
%    g_{ij} = d_{ij}p_{ij}
%\end{equation}
\begin{equation}
    \alpha_0(i) = \pi_{i}b_{s_i}(O_0) \\
\end{equation}
\begin{equation}
    \alpha_k(j) = \sum_{i=1}^{N} \alpha_{k-1}(i) v_{ij}(d_k) b_{s_j}(O_k);
    \quad 1 \leq k \leq L
\end{equation}
%\begin{equation}
%    \beta_L(i) = 1
%\end{equation}
%\begin{equation}
%    \beta_k(i) = \sum_{j=1}^{N} v_{ij}(d_k) b_{s_j}(O_{k+1}) \beta_{k+1}(j)
%\end{equation}
%\begin{equation}
%    \gamma_k(i) = \frac{\alpha_k(i) \beta_k(i)}
%        {\sum\limits_{i=1}^{N} \alpha_k(i) \beta_k(i)}
%\end{equation}
%\begin{equation}
%    \xi_k(i,j) =
%        \frac{\alpha_k(i) v_{ij}(d_{k+1}) b_{s_j}(O_{k+1}) \beta_{k+1}(j)}
%        {\sum\limits_{i=1}^{N} 
%            \alpha_k(i) v_{ij}(d_{k+1}) b_{s_j}(O_{k+1}) \beta_{k+1}(j)}
%\end{equation}
%\begin{equation}
%    \log\big [P(\boldsymbol{o}|\lambda)\big ] = -\sum_{k=1}^{N} \log\big [
%        \frac{1}{\sum\limits_{i=1}^{N} \alpha_k(i)} \big ]
%\end{equation}
%\begin{equation}
%    \pi_i = \gamma_0(i)
%\end{equation}
%\begin{equation}
%    b_i(o_j) = \frac{\sum\limits_{\substack{k=0 \\ O_k = o_j}}^{L} \gamma_k(i)}
%        {\sum\limits_{k=0}^{L} \gamma_k(i)}
%\end{equation}
%\begin{equation}
%    Q_i^v = \sum\limits_{k=1}^{L}
%        \bigg[\sum\limits_{\substack{i=1 \\ i \neq j}}^{N}
%        \Big\{ \xi_k(i, j) \log\big [p_{ij}d_{ij}(d_k)\big ]\Big\} +
%        \xi_k(i, i) \log \big [ 1 - \sum\limits_{\substack{h=1 \\ h \neq i}}^{N}
%        p_{ih}d_{ih}(d_k)\big ]\bigg ]
%\end{equation}
%\begin{equation}
%        \frac{\partial Q_i^v}{\partial p_{ij}} = \left\{
%        \begin{array}{l l}
%            \sum\limits_{k=1}^{L} \bigg [ \xi_k(i, j) \frac{1}{p_{ij}} - 
%                \xi_k(i, i) \frac{d_{ij}(d_k)}
%                {1 - \sum\limits_{\substack{h=1 \\ h \neq i}}^{N}
%                p_{ih}d_{ih}(d_k)} \bigg ]
%                & \quad \text{if $j \neq i$}\\
%            0
%                & \quad \text{if $j = i$}
%        \end{array} \right.
%\end{equation}
%\begin{equation}
%    \frac{\partial Q_i^v}{\partial \omega_{ij, r}} =
%        \frac{\partial Q_i^v}{\partial d_{ij}(d_k)}
%        \frac{\partial d_{ij}(d_k)}{\partial \omega_{ij, r}}
%\end{equation}
%\begin{equation}
%        \frac{\partial Q_i^v}{\partial d_{ij}(d_k)} = \left\{
%        \begin{array}{l l}
%            \sum\limits_{k=1}^{L} \bigg [ \xi_k(i, j) \frac{1}{d_{ij}(d_k)} -
%                \xi_k(i, i) \frac{p_{ij}}
%                {1 - \sum\limits_{\substack{h=1 \\ h \neq i}}^{N}
%                p_{ih}d_{ih}(d_k)} \bigg ]
%                & \quad \text{if $j \neq i$}\\
%            0
%                & \quad \text{if $j = i$}
%        \end{array} \right.
%\end{equation}
%\begin{equation}
%    \frac{\partial d_{ij}(d_k)}{\partial \omega_{ij, r}} =
%    \kappa_{ij, r}(d_k | \theta_{ij, r})
%\end{equation}
%\begin{equation}
%    \frac{\partial Q_i^v}{\partial \theta_{ij, r}} =
%        \frac{\partial Q_i^v}{\partial d_{ij}(d_k)}
%        \frac{\partial d_{ij}(d_k)}{\partial \kappa_{ij, r}}
%        \frac{\partial \kappa_{ij, r}}{\partial \theta_{ij, r}}
%\end{equation}
%\begin{equation}
%    \frac{\partial \kappa_{ij, gauss}(\mu_{ij}, \sigma_{ij})}
%        {\partial \mu_{ij}} =
%        \sum\limits_{k=1}^{L} \bigg [
%        \frac{\mu_{ij} - d_k}{\sigma_{ij}^3 \sqrt{2 \pi}}
%    \exp(-0.5 \frac{(d_k - \mu_{ij})^2}{\sigma_{ij}}) \bigg ]
%\end{equation}
%\begin{equation}
%    \frac{\partial \kappa_{ij, gauss}(\mu_{ij}, \sigma_{ij})}
%        {\partial \sigma_{ij}} =
%        \sum\limits_{k=1}^{L} \bigg [
%        \frac{-\sigma_{ij}^2 + \mu_{ij}^2 - 2 \mu_{ij} d_k + d_k^2}
%        {\sigma_{ij}^4 \sqrt{2 \pi}}
%    \exp(-0.5 \frac{(d_k - \mu_{ij})^2}{\sigma_{ij}}) \bigg ]
%\end{equation}

Description of D(t), P, G, pi, kernel, weights, distribution
function,

\begin{description} % Numbered list example
    \item[Data Processing] \hfill \\
        Tupling and grouping of events\dots
    \item[Model Training] \hfill \\
        Baum-Welch algorithm\dots
    \item[Sequence Processing] \hfill \\
        Forward algorithm. Sequence likelihood.
    \item[Classification] \hfill \\
        Bayes Decision.
\end{description} 

%----------------------------------------------------------------------------------------
%----------------------------------------------------------------------------------------

\section{Method, Design and Experiments} % Major section

In a first step, the proposed algorithm will be implemented in
Octave\footnote{https://www.gnu.org/software/octave}. The main reason for
choosing Octave is on one hand the simplicity to implement complex algorithms
with only a few lines of code and on the other hand the immediate revelation of
possible parallelization possibilities. This implementation is also meant to
help to fully understand the algorithm and possibly provide some ideas about
optimizations related to the design of an accelerator.

The next step will consist of generating synthetic data to verify the
correctness of the implementation. This data will also be used to verify
further implementations and to compute a speedup by comparing the accelerated
and the non-accelerated implementation. Knowing this, it is important to
generate data that can easily be imported into Octave but also been used as
input stream for a C/C++ implementation.

Further, the online part (sequence processing, classification) of the algorithm
will be implemented in C++. To be able to run and verify the algorithm, the
training data computed with Octave will be used. It is possible that due to
optimization decisions it will also be necessary to implement the off-line part
of the algorithm (training) in C++. In this case, the training data will be
computed anew. To efficiently implement the algorithm in C++ a linear algebra
library will be used (e.g. Armadillo\footnote{http://arma.sourceforge.net},
Eigen\footnote{http://eigen.tuxfamily.org},
LAPACK\footnote{http://www.netlib.org/lapack}, etc.).  It will be analyzed
which library is best fitting for the purposes of this work. This sequential
implementation will represent the reference to which the accelerated
implementation will be compared to.

Finally, the accelerator will be designed. The design will include discussion
concerning the choice of accelerator (e.g. FPGA, GPU, multiple cores). As
a minimum approach, only the online part of the algorithm will be accelerated.
However, due to optimization reasons it may be desirable to also accelerate the
off-line part and alter the whole algorithm in such away as to compute the
learning process online and hence being able to react to model aging problems.
Again the generated synthetic data will be used to verify the algorithm as well
as to compute a speedup compared to the non-accelerated implementation.

In addition to the steps listed above it would be very desirable to verify the
accelerated algorithm with real data. To accomplish this, one idea is to set up
the following experiment:

Run high-load procedures (e.g. prime number computation) on a undervolted
CPU-core and observe the internal hardware counters of the CPU-core. Whenever
a counter overflows, the counter id and the time of the overflow is recorded.
Additionally also the time and the type of failures of the CPU are recorded.
This data can then be fed to the algorithm:
\begin{itemize}
    \item each recorded counter id corresponds to an event
    \item the difference between two consecutive events corresponds to the
        delay $ d_k = t_{k+1} - t_k $
    \item the failure events are used as oracle to compute F-measure, Precision
        and Recall
\end{itemize}
Should the experiment result in a reasonable F-measure, it would not only have
been showed that the accelerated algorithm works on real data, but also that it
is possible to predict HW failures of a CPU-core by using hardware counters.
This experiment must first be performed with a non-optimized version of the
algorithm (i.e. the exact same implementation as described in \cite{salfner08})
before it is repeated with an optimized version (if any optimization has been
implemented).

%----------------------------------------------------------------------------------------
%----------------------------------------------------------------------------------------

\section{Preliminary Work Progress} % Major section
todo\dots

%----------------------------------------------------------------------------------------
%----------------------------------------------------------------------------------------

\section{Time Schedule} % Major section
todo\dots

%----------------------------------------------------------------------------------------
%	BIBLIOGRAPHY
%----------------------------------------------------------------------------------------

%\bibliographystyle{abbrvnat}
\bibliographystyle{siam}
\bibliography{biblio}

%----------------------------------------------------------------------------------------

\end{document}
