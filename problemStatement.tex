%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Simple Sectioned Essay Template
% LaTeX Template
%
% This template has been downloaded from:
% http://www.latextemplates.com
%
% Note:
% The \lipsum[#] commands throughout this template generate dummy text
% to fill the template out. These commands should all be removed when 
% writing essay content.
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%----------------------------------------------------------------------------------------
%	PACKAGES AND OTHER DOCUMENT CONFIGURATIONS
%----------------------------------------------------------------------------------------

\documentclass[12pt]{article} % Default font size is 12pt, it can be changed here

\usepackage[utf8]{inputenc}
\usepackage[square,sort,numbers]{natbib}
\usepackage[a4paper]{geometry} % Required to change the page size to A4
\usepackage{mathtools}
\usepackage{hyperref}

\begin{document}

%----------------------------------------------------------------------------------------
%	TITLE PAGE
%----------------------------------------------------------------------------------------

\begin{titlepage}

\newcommand{\HRule}{\rule{\linewidth}{0.5mm}} % Defines a new command for the horizontal lines, change thickness here

\center % Center everything on the page

\textsc{\LARGE UniversitÃ  della Svizzera Italiana}\\[1.5cm] % Name of your university/college
\textsc{\Large Master Thesis}\\[0.5cm] % Major heading such as course name
\textsc{\large Accelerator for Event-based Failure Prediction}\\[0.5cm] % Minor heading such as course title

\HRule \\[0.4cm]
{ \huge \bfseries Problem Statement}\\[0.4cm] % Title of your document
\HRule \\[1.5cm]

\begin{minipage}{0.4\textwidth}
\begin{flushleft} \large
\emph{Author:}\\
Simon \textsc{Maurer} % Your name
\end{flushleft}
\end{minipage}
~
\begin{minipage}{0.4\textwidth}
\begin{flushright} \large
\emph{Supervisor:} \\
Prof. Miroslaw \textsc{Malek} % Supervisor's Name
\end{flushright}
\end{minipage}\\[4cm]

{\large \today}\\[3cm] % Date, change the \today to a set date if you want to be precise

\vfill % Fill the rest of the page with whitespace

\end{titlepage}

%----------------------------------------------------------------------------------------
%	TABLE OF CONTENTS
%----------------------------------------------------------------------------------------

\tableofcontents % Include a table of contents

\newpage % Begins the essay on a new page instead of on the same page as the table of contents 

%----------------------------------------------------------------------------------------
%	INTRODUCTION
%----------------------------------------------------------------------------------------

\section{Introduction} % Major section

This document provides an outline of the planned work for the master thesis
"Accelerator for Event-based Failure Prediction". The document addresses the
problem considered by the thesis, a state of the art outline as well as
important questions and hypothesis that arise or are needed for the thesis.
Further the document provides a list of necessary theory bases and introduces
the design and experimenting methods applied in the thesis. Finally, the
document gives an overview of necessary material in terms of literature and
equipment and provides a broad time
schedule.

%----------------------------------------------------------------------------------------
%----------------------------------------------------------------------------------------

\section{Problem and Research Area} % Major section

In today's live it becomes increasingly important, that computer systems are
dependable. The reason being, that computer systems are used more and more in
areas where the failure of such a system can lead to catastrophic events.
Banking, public transportation and medical engineering are only a few examples
of areas employing large and extremely complex systems. The increasing
complexity of computer systems has a direct impact on their maintainability and
testability. It is simply impossible to guarantee that a piece of software comes
without any faults. On top of that, the same problematic arises with the
hardware components which also may contain faulty parts but also get
increasingly prone to failures due to decay of material.

In the event of a system failure it is of course desirable to fix the system as
soon as possible in order to minimize the downtime of the system (maximize the
availability). This can be accomplished by using different types of recovery
techniques, e.g. Check-pointing (create checkpoints to roll back/forward),
system replication (switch to a redundant system), fail over (reboot). All these
techniques require a certain amount of time to complete the recovery process,
time that is very expensive. In order to minimize this time, techniques have
been developed to anticipate upcoming failures. Such a technique is described in
\cite{salfner08}.

The work presents a new algorithm to predict failures and compares the results
with other techniques. The accuracy of the presented algorithm to predict
failures proves to be better compared to the other techniques, has however the
drawback of increased complexity and hence increased computation time. It is
very important to keep the computation overhead very low in order to maximize
the time between the prediction of a failure and the actual event of the
failure. One way to decrease the computation time is to design a hardware
accelerator for the prediction algorithm. The design of such an accelerator is
outlined in this document.

%----------------------------------------------------------------------------------------
%----------------------------------------------------------------------------------------

\section{State of the Art Outline} % Major section

This section provides a brief outline of the state of the art in the different
fields of research that are relevant for the thesis. This includes a small
overview of failure prediction methods and existing solutions to accelerate
failure prediction algorithms.

%------------------------------------------------

\subsection{Failure Prediction} % Sub-section

A very detailed overview of failure prediction methods is given in
\cite{ACM10_Salfner}. The survey discusses i.a. the techniques used as
comparison in the main reference
\cite{lin88,IEEE90_lin,ICDM02_Vilalta,domeniconi02} as well as the technique
described in the main reference \cite{salfner08}.

More recent work uses hardware counters of a general purpose CPU and combines
them with software instrumentation to predict failures of single processes (e.g
grep, flex, sed) \cite{FSE10_Yilmaz,ozcelik14}. As industry heads more and more
towards cloud computing, it has been proposed to use information of interaction
between nodes (instead of analyzing single nodes) in order to analyze and
predict failures of a distributed system \cite{IEEE12_Salfner,DSN10_Oliner}.

%------------------------------------------------

\subsection{Accelerator} % Sub-section

The main goal of this master thesis is to accelerate an adaptation of the
forward algorithm. Proposals for a GPU based accelerator for the classic
forward algorithm are described in \cite{neumann11,liu09}. Further, several
proposals to accelerate the Viterbi algorithm (which is closely related to the
forward algorithm) have been published: \cite{ASAP12_Azhar} presents an
architecture for a lightweight Viterbi accelerator designed for an embedded
processor datapath, \cite{IPDPS07_Jacob,ICS06_Maddimsetty,IPDPS07_Oliver}
describe a FPGA based accelerator for protein sequence HHM search and
\cite{IPDPS09_Walters} describes i.a. an approach to accelerate the Viterbi
algorithm from the HMMER library using GPUs.

Focusing on a more general approach for acceleration, \cite{ARITH13_Kadric}
proposes an FPGA implementation of a parallel floating point accumulation and
\cite{ITNG07_Hongyan} describes the implementation of a vector processor on
FPGA.

Quite a few research has been done on the question what type of technology
should be used to accelerate certain algorithms: \cite{SASP08_Che} presents
a performance study of different applications accelerated on a multicore CPU,
on a GPU and on a FPGA, \cite{FPL10_Jones} discusses the suitability of FPGA
and GPU acceleration for high productivity computing systems (HPCS) without
focusing on a specific application and \cite{ISVLSI10_Kestur} also focuses on
HPCS but uses the Basic Linear Algebra Subroutines (BLAS) as comparison and
also takes CPUs into account.

It may be interesting to also think about an acceleration of the model
training. Similar work has been done by accelerating SVMs (Support Vector Machines):
\cite{FCCM09_Cadambi} describes a FPGA based accelerator for the SVM-SMO
(support vector machine - sequential minimal optimization) algorithm used in
the domain of machine learning and \cite{IEEE03_Anguita} proposes a new algorithm
and its implementation on a FPGA for SVMs.

%----------------------------------------------------------------------------------------
%----------------------------------------------------------------------------------------

\section{Questions and Hypothesis} % Major section

This section first lists the main questions that will be discussed, answered and
verified in the master thesis:
\begin{description}
    \item[Argumentation for the necessity of the work] \hfill \\
        The thesis will state why this work is necessary. Problems such as the
        huge amount of preliminary parametrization or the high computation
        effort will be discussed.
    \item[How to parallelize the algorithm] \hfill \\
        A theoretical analysis of the algorithm will identify the parts that can
        be parallelized and by using Amdahl's law the theoretical possible
        speedup will be computed.
    \item[Choice of accelerator technology] \hfill \\
        Different types of accelerators must be evaluated under the
        consideration of the properties of the adapted forward algorithm.
        Parameters like available parallelization, required dataflow, initial
        parametrization, different cost metrics, etc. need to be taken into
        account. Possible accelerator technologies are multicore processors,
        GPUs and FPGAs.
    \item[Practical implementation of the accelerator] \hfill \\
        Using the theoretical analysis of the algorithm as well as the choice
        of the accelerator type, practical decisions upon the type of
        acceleration techniques will be taken and discussed.
    \item[How to generate test data to verify the accelerator] \hfill \\
        In order to verify the work, test data needs to be available. Either the
        data is taken from some public repository, or it must be generated.
        Problematic is the fact, that algorithm-specific data is necessary. This
        problem will be discussed.
\end{description}

Second the hypotheses are stated, that will be taken into account to design the
accelerator for the adapted forward algorithm:
\begin{itemize}
    \item The reference algorithm is considered to be verified and tested. The
        verification in the master thesis will only focus on a valid
        theoretical and experimental computation of the speedup by comparing the
        accelerated version of the adapted forward algorithm with the serial
        computation on a CPU.
    \item The system using the accelerator provides standardized log events.
        The master thesis is not discussing the nature of these events. It is
        supposed, that the events are generated sequentially and fed to the
        accelerator through a defined interface (to be defined in the thesis).
    \item Only the online part of the reference algorithm will be accelerated.
        The acceleration of model training which is computed offline is optional
        and will most likely be neglected in this master thesis.
\end{itemize}

%----------------------------------------------------------------------------------------
%----------------------------------------------------------------------------------------

\section{Theory Base} % Major section

This section provides a brief overview of the computational steps done by the
proposed algorithm. The following description should provide a complete
blueprint of the adapted forward algorithm, that allows to implement it, but
without any explications or proofs related to the formulation. To be able to
understand the formal expression of the algorithm, first a definition of the
used parameters is provided.
\begin{itemize}
    \item N: number of states
    \item M: number of observation symbols
    \item L: observation sequence length
    \item R: number of cumulative probability distributions (kernels)
\end{itemize}
The delay of the event at time $ t_k $ with respect to the event at time
$ t_{k-1} $ is described as
\begin{equation}
    d_k = t_k-t_{k-1}
\end{equation}
One part of the algorithm is the model training. This part is not described
here. The features to be trained by the model training are however important
in this context because they are used by the adapted forward algorithm.
Following the features:
\begin{itemize}
    \item $ \pi_i $, forming the initial state probability vector
        $ \boldsymbol{\pi} $ of size $ N $
    \item $ b_i(o_j) $, forming the emission probability matrix $ B $ of size
        $ N \times M $
    \item $ p_{ij} $, forming the matrix of limiting transmission probabilities
        $ P $ of size $ N \times N $
    \item $ \omega_{ij, r} $, the weights of the kernel $ r $
    \item $ \theta_{ij, r} $, the parameters of the kernel $ r $
\end{itemize}
For simplification reasons, only one kernel is used. Due to this, the kernel
weights can be ignored. Choosing the gaussian cumulative distribution results
in the kernel parameters $ \mu_{ij} $ and $ \sigma_{ij} $.  The adapted forward
algorithm is defined as follows:
\begin{equation}
    \alpha_0(i) = \pi_{i}b_{s_i}(O_0) \\
\end{equation}
\begin{equation}
    \alpha_k(j) = \sum_{i=1}^{N} \alpha_{k-1}(i) v_{ij}(d_k) b_{s_j}(O_k);
    \quad 1 \leq k \leq L
\end{equation}
where
\begin{equation}
    v_{ij}(d_k) = \left\{
        \begin{array}{l l}
            p_{ij} d_{ij}(d_k)
                & \quad \text{if $j \neq i$}\\
            1 - \sum\limits_{\substack{h=1 \\ h \neq i}}^{N} p_{ih} d_{ih}(d_k)
                & \quad \text{if $j = i$}
        \end{array} \right.
\end{equation}
with
\begin{equation}
    d_{ij}(d_k) = \sum_{r=1}^{R} \omega_{ij,r}\kappa_{ij,r}(d_k|\theta_{ij, r})
\end{equation}
forming the matrix of cumulative transition duration distribution functions
$ D(d_k) $ of size $ N \times N \times L $, and
%\begin{equation}
%    \kappa_{ij, gauss}(d_k | \mu_{ij}, \sigma_{ij}) = 
%        \frac{1}{\sigma_{ij} \sqrt{2 \pi}}
%        \exp(-0.5 \frac{(d_k - \mu_{ij})^2}{\sigma_{ij}^2})
%\end{equation}
\begin{equation}
    \kappa_{ij, gauss}(d_k | \mu_{ij}, \sigma_{ij}) = 
        0.5\bigg [1 + \frac{2}{\pi}\Big (
            1 - \exp \big (-\frac{d_k - \mu_{ij}}{\sqrt 2 \sigma_{ij}}\big )
        \Big ) \bigg ]
\end{equation}
To prevent $ \alpha $ from going to zero very fast, at each step of the forward
algorithm a scaling is performed:
\begin{equation}
    \alpha_k(i) = c_k \alpha_k(i)
\end{equation}
with
\begin{equation}
    c_k = \frac{1}{\sum\limits_{i=1}^{N} \alpha_k(i)}
\end{equation}
then the sequence log-likelihood is computed:
\begin{equation}
    \log(P(\boldsymbol{o}|\lambda)) = -\sum\limits_{k=1}^{L} \log c_k
\end{equation}
where $ \lambda = \{\boldsymbol{\pi}, P, B, D(d_k) \} $,  and finally the
classification is performed:
\begin{equation}
    \text{class}(s) = F \iff \max_{i=1}^{u} \big [
        \log P(\boldsymbol{s}|\lambda_i)
    \big ] - \log P(\boldsymbol{s}|\lambda_0) > \log \theta
\end{equation}
with
\begin{equation}
    \theta = \frac{(r_{\bar{F}F} - r_{\bar{F}\bar{F}})P(c_{\bar{F}})}
        {(r_{F \bar{F}} - r_{FF})P(c_{F})}
\end{equation}
To calculate $ \theta $, the following parameters need to be set:
\begin{itemize}
    \item $ P(c_{\bar{F}}) $: prior of non-failure class
    \item $ P(c_F) $: prior of failure class
    \item $ r_{\bar{F}\bar{F}} $: true negative prediction
    \item $ r_{FF} $: true positive prediction
    \item $ r_{\bar{F}F} $: false positive prediction
    \item $ r_{F\bar{F}} $: false negative prediction
\end{itemize}
%\begin{equation}
%    g_{ij} = d_{ij}p_{ij}
%\end{equation}
%\begin{equation}
%    \beta_L(i) = 1
%\end{equation}
%\begin{equation}
%    \beta_k(i) = \sum_{j=1}^{N} v_{ij}(d_k) b_{s_j}(O_{k+1}) \beta_{k+1}(j)
%\end{equation}
%\begin{equation}
%    \gamma_k(i) = \frac{\alpha_k(i) \beta_k(i)}
%        {\sum\limits_{i=1}^{N} \alpha_k(i) \beta_k(i)}
%\end{equation}
%\begin{equation}
%    \xi_k(i,j) =
%        \frac{\alpha_k(i) v_{ij}(d_{k+1}) b_{s_j}(O_{k+1}) \beta_{k+1}(j)}
%        {\sum\limits_{i=1}^{N} 
%            \alpha_k(i) v_{ij}(d_{k+1}) b_{s_j}(O_{k+1}) \beta_{k+1}(j)}
%\end{equation}
%\begin{equation}
%    \log\big [P(\boldsymbol{o}|\lambda)\big ] = -\sum_{k=1}^{N} \log\big [
%        \frac{1}{\sum\limits_{i=1}^{N} \alpha_k(i)} \big ]
%\end{equation}
%\begin{equation}
%    \pi_i = \gamma_0(i)
%\end{equation}
%\begin{equation}
%    b_i(o_j) = \frac{\sum\limits_{\substack{k=0 \\ O_k = o_j}}^{L} \gamma_k(i)}
%        {\sum\limits_{k=0}^{L} \gamma_k(i)}
%\end{equation}
%\begin{equation}
%    Q_i^v = \sum\limits_{k=1}^{L}
%        \bigg[\sum\limits_{\substack{i=1 \\ i \neq j}}^{N}
%        \Big\{ \xi_k(i, j) \log\big [p_{ij}d_{ij}(d_k)\big ]\Big\} +
%        \xi_k(i, i) \log \big [ 1 - \sum\limits_{\substack{h=1 \\ h \neq i}}^{N}
%        p_{ih}d_{ih}(d_k)\big ]\bigg ]
%\end{equation}
%\begin{equation}
%        \frac{\partial Q_i^v}{\partial p_{ij}} = \left\{
%        \begin{array}{l l}
%            \sum\limits_{k=1}^{L} \bigg [ \xi_k(i, j) \frac{1}{p_{ij}} - 
%                \xi_k(i, i) \frac{d_{ij}(d_k)}
%                {1 - \sum\limits_{\substack{h=1 \\ h \neq i}}^{N}
%                p_{ih}d_{ih}(d_k)} \bigg ]
%                & \quad \text{if $j \neq i$}\\
%            0
%                & \quad \text{if $j = i$}
%        \end{array} \right.
%\end{equation}
%\begin{equation}
%    \frac{\partial Q_i^v}{\partial \omega_{ij, r}} =
%        \frac{\partial Q_i^v}{\partial d_{ij}(d_k)}
%        \frac{\partial d_{ij}(d_k)}{\partial \omega_{ij, r}}
%\end{equation}
%\begin{equation}
%        \frac{\partial Q_i^v}{\partial d_{ij}(d_k)} = \left\{
%        \begin{array}{l l}
%            \sum\limits_{k=1}^{L} \bigg [ \xi_k(i, j) \frac{1}{d_{ij}(d_k)} -
%                \xi_k(i, i) \frac{p_{ij}}
%                {1 - \sum\limits_{\substack{h=1 \\ h \neq i}}^{N}
%                p_{ih}d_{ih}(d_k)} \bigg ]
%                & \quad \text{if $j \neq i$}\\
%            0
%                & \quad \text{if $j = i$}
%        \end{array} \right.
%\end{equation}
%\begin{equation}
%    \frac{\partial d_{ij}(d_k)}{\partial \omega_{ij, r}} =
%    \kappa_{ij, r}(d_k | \theta_{ij, r})
%\end{equation}
%\begin{equation}
%    \frac{\partial Q_i^v}{\partial \theta_{ij, r}} =
%        \frac{\partial Q_i^v}{\partial d_{ij}(d_k)}
%        \frac{\partial d_{ij}(d_k)}{\partial \kappa_{ij, r}}
%        \frac{\partial \kappa_{ij, r}}{\partial \theta_{ij, r}}
%\end{equation}
%\begin{equation}
%    \frac{\partial \kappa_{ij, gauss}(\mu_{ij}, \sigma_{ij})}
%        {\partial \mu_{ij}} =
%        \sum\limits_{k=1}^{L} \bigg [
%        \frac{\mu_{ij} - d_k}{\sigma_{ij}^3 \sqrt{2 \pi}}
%    \exp(-0.5 \frac{(d_k - \mu_{ij})^2}{\sigma_{ij}}) \bigg ]
%\end{equation}
%\begin{equation}
%    \frac{\partial \kappa_{ij, gauss}(\mu_{ij}, \sigma_{ij})}
%        {\partial \sigma_{ij}} =
%        \sum\limits_{k=1}^{L} \bigg [
%        \frac{-\sigma_{ij}^2 + \mu_{ij}^2 - 2 \mu_{ij} d_k + d_k^2}
%        {\sigma_{ij}^4 \sqrt{2 \pi}}
%    \exp(-0.5 \frac{(d_k - \mu_{ij})^2}{\sigma_{ij}}) \bigg ]
%\end{equation}

%----------------------------------------------------------------------------------------
%----------------------------------------------------------------------------------------

\section{Method, Design and Experiments} % Major section

In a first step, the proposed algorithm will be implemented in
Octave\footnote{https://www.gnu.org/software/octave}. The main reason for
choosing Octave is on one hand the simplicity to implement complex algorithms
with only a few lines of code and on the other hand the immediate revelation of
parallelization possibilities. This implementation is also meant to help to
fully understand the algorithm and possibly provide some ideas about
optimizations related to the design of an accelerator.

The next step will consist of generating synthetic data to verify the
correctness of the implementation. This data will also be used to verify
further implementations and to compute a speedup by comparing the accelerated
and the non-accelerated implementation. Knowing this, it is important to
generate data that can easily be imported into Octave but also been used as
input stream for a C/C++ implementation.

Further, the online part (sequence processing, classification) of the algorithm
will be implemented in C++. To be able to run and verify the algorithm, the
training data computed with Octave will be used. It is possible that due to
optimization decisions it will also be necessary to implement the off-line part
of the algorithm (training) in C++. In this case, the training data will be
computed anew. To efficiently implement the algorithm in C++ a linear algebra
library will be used (e.g. Armadillo\footnote{http://arma.sourceforge.net},
Eigen\footnote{http://eigen.tuxfamily.org},
LAPACK\footnote{http://www.netlib.org/lapack}, etc.).  It will be analyzed
which library is best fitting for the purposes of this work. This sequential
implementation will represent the reference to which the accelerated
implementation will be compared to.

Finally, the accelerator will be designed. The design will include discussion
concerning the choice of accelerator (e.g. FPGA, GPU, multiple cores). As
a minimum approach, only the online part of the algorithm will be accelerated.
However, due to optimization reasons it may be desirable to also accelerate the
off-line part and alter the whole algorithm in such away as to compute the
learning process online and hence being able to react to model aging problems.
Again the generated synthetic data will be used to verify the algorithm as well
as to compute a speedup compared to the non-accelerated implementation.

In addition to the steps listed above it would be very desirable to verify the
accelerated algorithm with real data. To accomplish this, one idea is to set up
the following experiment:

Run high-load procedures (e.g. prime number computation) on a undervolted
CPU-core and observe the internal hardware counters of the CPU-core. Whenever
a counter overflows, the counter id and the time of the overflow is recorded.
Additionally also the time and the type of failures of the CPU are recorded.
This data can then be fed to the algorithm:
\begin{itemize}
    \item each recorded counter id corresponds to an event
    \item the difference between two consecutive events corresponds to the
        delay $ d_k = t_{k+1} - t_k $
    \item the failure events are used as oracle to compute F-measure, Precision
        and Recall
\end{itemize}
Should the experiment result in a reasonable F-measure, it would not only have
been showed that the accelerated algorithm works on real data, but also that it
is possible to predict HW failures of a CPU-core by using hardware counters.
This experiment must first be performed with a non-optimized version of the
algorithm (i.e. the exact same implementation as described in \cite{salfner08})
before it is repeated with an optimized version (if any optimization has been
implemented).

Note that there are strong doubts that the speedup of the accelerator will be
enough to actually perform the experiment mentioned above. Most likely the
events will occur far to fast for the algorithm to compute the result in time.

%----------------------------------------------------------------------------------------
%----------------------------------------------------------------------------------------

\section{Time Schedule} % Major section
This section provides a broad overview of the time schedule to finish the
master thesis. This schedule is not accurate and serves only as a guideline. It
will certainly change due to the fact, that the date of the defense is not yet
available.

\begin{description}
    \item[End of March] \hfill \\
        The decision on the type of accelerator has been taken. The process to
        get to this decision involves the study of the state of the art in
        accelerator technologies and the study of the algorithm to accelerate.
    \item[Mid of April] \hfill \\
        A verification scheme is available as well as the data to verify the
        accelerator. This also includes the exact choice of the device on which
        the accelerator will be implemented and tested.
    \item[Mid of Mai] \hfill \\
        The final implementation of the accelerator is available as well as
        a serial implementation serving as a reference. The verification phase
        can start. Until this point all acceleration decisions have been
        discussed and documented.
    \item[Mid of June] \hfill \\
        The first draft of the documentation is available.
    \item[End of June] \hfill \\
        The documentation has been finalized.
\end{description}
%----------------------------------------------------------------------------------------
%	BIBLIOGRAPHY
%----------------------------------------------------------------------------------------

%\bibliographystyle{abbrvnat}
\bibliographystyle{siam}
\bibliography{biblio}

%----------------------------------------------------------------------------------------

\end{document}
